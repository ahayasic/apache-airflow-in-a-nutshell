{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Apache Airflow in a Nutshell \u00b6 Este reposit\u00f3rio cont\u00e9m um conjunto de arquivos markdown que explicam sobre a ferramenta Apache Airflow, incluindo boas pr\u00e1ticas e receitas. O trabalho ainda est\u00e1 em progresso. Portanto, erros ser\u00e3o corrigidos e melhorias adicionadas \u00e0 medida que o trabalho avan\u00e7ar. Caso queira contribuir com sugest\u00f5es, sinta-se \u00e0 vontade para abrir Issues & PRs! Por fim, este projeto tem como refer\u00eancias principais os seguintes recursos (sendo o primeiro o mais importante): Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Apache Airflow Documentation Airflow Guides by Astronomer Marc Lamberti Blog","title":"In\u00edcio"},{"location":"#apache-airflow-in-a-nutshell","text":"Este reposit\u00f3rio cont\u00e9m um conjunto de arquivos markdown que explicam sobre a ferramenta Apache Airflow, incluindo boas pr\u00e1ticas e receitas. O trabalho ainda est\u00e1 em progresso. Portanto, erros ser\u00e3o corrigidos e melhorias adicionadas \u00e0 medida que o trabalho avan\u00e7ar. Caso queira contribuir com sugest\u00f5es, sinta-se \u00e0 vontade para abrir Issues & PRs! Por fim, este projeto tem como refer\u00eancias principais os seguintes recursos (sendo o primeiro o mais importante): Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Apache Airflow Documentation Airflow Guides by Astronomer Marc Lamberti Blog","title":"Apache Airflow in a Nutshell"},{"location":"about/","text":"Sobre o Autor \u00b6 Ol\u00e1, meu nome \u00e9 Alisson! Tenho 24 anos, estudo Ci\u00eancia da Computa\u00e7\u00e3o na Universidade Federal S\u00e3o Carlos e atualmente trabalho como Engenheiro de Dados no iFood . Como voc\u00ea j\u00e1 deve saber, ensinar \u00e9 a melhor forma de aprender e este projeto tem exatamente essa inten\u00e7\u00e3o! Todo o conte\u00fado presente aqui tem como objetivo ser um caderno de notas e resumos \\(-\\) de guia pessoal \\(-\\) sobre os principais t\u00f3picos relacionados ao Apache Airflow. Al\u00e9m disso, dado o car\u00e1ter pessoal, pe\u00e7o que n\u00e3o se estresse ou fique chateado caso a did\u00e1tica seja ruim e o conte\u00fado esteja mal organizado e faltante, afinal o conte\u00fado n\u00e3o visa servir como refer\u00eancia ou material de ensino, mas sim como um guia pessoal! Por\u00e9m, ao mesmo tempo, caso encontre quaisquer erros, por favor entre em contato comigo e irei corrigi-los o mais r\u00e1pido poss\u00edvel (e voc\u00ea tamb\u00e9m estar\u00e1 me ajudando a aprender mais!). Enfim, divirta-se navegando pelo meu conhecimento!","title":"Sobre o Autor"},{"location":"about/#sobre-o-autor","text":"Ol\u00e1, meu nome \u00e9 Alisson! Tenho 24 anos, estudo Ci\u00eancia da Computa\u00e7\u00e3o na Universidade Federal S\u00e3o Carlos e atualmente trabalho como Engenheiro de Dados no iFood . Como voc\u00ea j\u00e1 deve saber, ensinar \u00e9 a melhor forma de aprender e este projeto tem exatamente essa inten\u00e7\u00e3o! Todo o conte\u00fado presente aqui tem como objetivo ser um caderno de notas e resumos \\(-\\) de guia pessoal \\(-\\) sobre os principais t\u00f3picos relacionados ao Apache Airflow. Al\u00e9m disso, dado o car\u00e1ter pessoal, pe\u00e7o que n\u00e3o se estresse ou fique chateado caso a did\u00e1tica seja ruim e o conte\u00fado esteja mal organizado e faltante, afinal o conte\u00fado n\u00e3o visa servir como refer\u00eancia ou material de ensino, mas sim como um guia pessoal! Por\u00e9m, ao mesmo tempo, caso encontre quaisquer erros, por favor entre em contato comigo e irei corrigi-los o mais r\u00e1pido poss\u00edvel (e voc\u00ea tamb\u00e9m estar\u00e1 me ajudando a aprender mais!). Enfim, divirta-se navegando pelo meu conhecimento!","title":"Sobre o Autor"},{"location":"content/admin_ops/operating_airflow/","text":"Work in progress","title":"Administrando o Airflow"},{"location":"content/admin_ops/running_airflow_in_the_clouds/","text":"Work in progress","title":"Executando o Airflow na Nuvem"},{"location":"content/admin_ops/running_airflow_locally/","text":"Work in progress","title":"Executando o Aiflow Localmente"},{"location":"content/best_practices/best_practices/","text":"Work in progress","title":"Boas Pr\u00e1ticas"},{"location":"content/best_practices/recipes/","text":"Receitas & C\u00f3digos Padr\u00f5es \u00b6 Configurando o Agendamento de DAGs \u00b6 O agendamento de DAGs \u00e9 simples e dividido em duas etapas: Agendamento da DAG Escalonamento das Tarefas Logo, no geral precisamos responder as seguintes perguntas: A partir de qual momento a DAG deve ser escalonada? Com qual frequ\u00eancia a DAG deve ser executada? Se manual, atribuir None . Se poss\u00edvel utilizar cron , utilizar cron. Caso contr\u00e1rio, utilizar datetime . Qual crit\u00e9rio condicional para disparar a DAG? Em que momento a DAG n\u00e3o deve mais ser executada? As tasks dependem do sucesso das execu\u00e7\u00f5es passadas? (Ou dependem de tasks passadas, no geral) Se sim, usar depends_on_past=True Qual o tempo m\u00e1ximo que uma task pode se manter em execu\u00e7\u00e3o? Se uma task falhar: (Enquanto a DAG ainda estiver em execu\u00e7\u00e3o) Quantas vezes a task pode tentar ser reexecutada? As execu\u00e7\u00f5es passadas devem ser refeitas? Se sim, use catchup=True . Caso contr\u00e1rio, use False Nota As perguntas s\u00e3o apenas para orienta\u00e7\u00e3o e n\u00e3o necessariamente algo obrigat\u00f3rio. Exemplo \u00b6 A partir de qual momento a DAG deve ser escalonada? A DAG deve ser executada a partir do dia 20 de Setembro de 2021. Com qual frequ\u00eancia a DAG deve ser executada? A DAG deve ser executada a cada dois dias \u00e0s 9hrs. Qual crit\u00e9rio condicional para disparar a DAG? Nenhum. Em que momento a DAG n\u00e3o deve mais ser executada? Nenhum. As tasks dependem do sucesso das execu\u00e7\u00f5es passadas? N\u00e3o. Qual o tempo m\u00e1ximo que uma task pode se manter em execu\u00e7\u00e3o? Padr\u00e3o. Se uma task falhar, quantas vezes a task pode tentar ser reexecutada? Padr\u00e3o. Se uma task falhar, as execu\u00e7\u00f5es passadas devem ser refeitas? N\u00e3o. from datetime import datetime from airflow.models import DAG dag = DAG ( start_date = datetime ( 2021 , 9 , 20 ), schedule_interval = \"0 9 */2 * *\" , catchup = False , default_args = { \"depends_on_past\" : False }, ) Dicas Adicionais \u00b6 Catchup & Backfilling \u00b6 Ao combinarmos depends_on_past=True e catchup=True , execu\u00e7\u00f5es passadas de uma tarefa que falhou ser\u00e3o reexecutadas, automaticamente, de forma ordenada. Dica Podemos for\u00e7ar a reexecu\u00e7\u00e3o de tarefas passadas atrav\u00e9s do bot\u00e3o Clear . Se definirmos depends_on_past=True e catchup=False , execu\u00e7\u00f5es passadas n\u00e3o ser\u00e3o refeitas automaticamente. Por\u00e9m, ainda podemos for\u00e7ar a reexecu\u00e7\u00e3o que, no caso acontecer\u00e1 de forma ordenada. Ao mesmo tempo, note que ao definirmos depends_on_past=True para uma tarefa e ela falhar, execu\u00e7\u00f5es posteriores desta mesma tarefa n\u00e3o ir\u00e3o acontecer automaticamente . Se definirmos depends_on_past=False e catchup=True , tasks que falharam ser\u00e3o reexecutadas mas de forma desordenada. J\u00e1 se depends_on_past=False e catchup=False , n\u00e3o haver\u00e1 qualquer reexecu\u00e7\u00e3o de tarefa ou depend\u00eancias. Portanto, caso a depend\u00eancia com tasks passadas n\u00e3o seja um comportamento desejado, mas a reexecu\u00e7\u00e3o de tarefas passadas (e.g. processamento) seja desejada eventualmente, o procedimento a ser seguido \u00e9: Alterar os valores de depends_on_past e catchup . Executar as tarefas. Voltar as configura\u00e7\u00f5es para a forma inicial. Configurando Sensores \u00b6 Configurando o PythonOperator \u00b6","title":"Receitas"},{"location":"content/best_practices/recipes/#receitas-codigos-padroes","text":"","title":"Receitas &amp; C\u00f3digos Padr\u00f5es"},{"location":"content/best_practices/recipes/#configurando-o-agendamento-de-dags","text":"O agendamento de DAGs \u00e9 simples e dividido em duas etapas: Agendamento da DAG Escalonamento das Tarefas Logo, no geral precisamos responder as seguintes perguntas: A partir de qual momento a DAG deve ser escalonada? Com qual frequ\u00eancia a DAG deve ser executada? Se manual, atribuir None . Se poss\u00edvel utilizar cron , utilizar cron. Caso contr\u00e1rio, utilizar datetime . Qual crit\u00e9rio condicional para disparar a DAG? Em que momento a DAG n\u00e3o deve mais ser executada? As tasks dependem do sucesso das execu\u00e7\u00f5es passadas? (Ou dependem de tasks passadas, no geral) Se sim, usar depends_on_past=True Qual o tempo m\u00e1ximo que uma task pode se manter em execu\u00e7\u00e3o? Se uma task falhar: (Enquanto a DAG ainda estiver em execu\u00e7\u00e3o) Quantas vezes a task pode tentar ser reexecutada? As execu\u00e7\u00f5es passadas devem ser refeitas? Se sim, use catchup=True . Caso contr\u00e1rio, use False Nota As perguntas s\u00e3o apenas para orienta\u00e7\u00e3o e n\u00e3o necessariamente algo obrigat\u00f3rio.","title":"Configurando o Agendamento de DAGs"},{"location":"content/best_practices/recipes/#exemplo","text":"A partir de qual momento a DAG deve ser escalonada? A DAG deve ser executada a partir do dia 20 de Setembro de 2021. Com qual frequ\u00eancia a DAG deve ser executada? A DAG deve ser executada a cada dois dias \u00e0s 9hrs. Qual crit\u00e9rio condicional para disparar a DAG? Nenhum. Em que momento a DAG n\u00e3o deve mais ser executada? Nenhum. As tasks dependem do sucesso das execu\u00e7\u00f5es passadas? N\u00e3o. Qual o tempo m\u00e1ximo que uma task pode se manter em execu\u00e7\u00e3o? Padr\u00e3o. Se uma task falhar, quantas vezes a task pode tentar ser reexecutada? Padr\u00e3o. Se uma task falhar, as execu\u00e7\u00f5es passadas devem ser refeitas? N\u00e3o. from datetime import datetime from airflow.models import DAG dag = DAG ( start_date = datetime ( 2021 , 9 , 20 ), schedule_interval = \"0 9 */2 * *\" , catchup = False , default_args = { \"depends_on_past\" : False }, )","title":"Exemplo"},{"location":"content/best_practices/recipes/#dicas-adicionais","text":"","title":"Dicas Adicionais"},{"location":"content/best_practices/recipes/#catchup-backfilling","text":"Ao combinarmos depends_on_past=True e catchup=True , execu\u00e7\u00f5es passadas de uma tarefa que falhou ser\u00e3o reexecutadas, automaticamente, de forma ordenada. Dica Podemos for\u00e7ar a reexecu\u00e7\u00e3o de tarefas passadas atrav\u00e9s do bot\u00e3o Clear . Se definirmos depends_on_past=True e catchup=False , execu\u00e7\u00f5es passadas n\u00e3o ser\u00e3o refeitas automaticamente. Por\u00e9m, ainda podemos for\u00e7ar a reexecu\u00e7\u00e3o que, no caso acontecer\u00e1 de forma ordenada. Ao mesmo tempo, note que ao definirmos depends_on_past=True para uma tarefa e ela falhar, execu\u00e7\u00f5es posteriores desta mesma tarefa n\u00e3o ir\u00e3o acontecer automaticamente . Se definirmos depends_on_past=False e catchup=True , tasks que falharam ser\u00e3o reexecutadas mas de forma desordenada. J\u00e1 se depends_on_past=False e catchup=False , n\u00e3o haver\u00e1 qualquer reexecu\u00e7\u00e3o de tarefa ou depend\u00eancias. Portanto, caso a depend\u00eancia com tasks passadas n\u00e3o seja um comportamento desejado, mas a reexecu\u00e7\u00e3o de tarefas passadas (e.g. processamento) seja desejada eventualmente, o procedimento a ser seguido \u00e9: Alterar os valores de depends_on_past e catchup . Executar as tarefas. Voltar as configura\u00e7\u00f5es para a forma inicial.","title":"Catchup &amp; Backfilling"},{"location":"content/best_practices/recipes/#configurando-sensores","text":"","title":"Configurando Sensores"},{"location":"content/best_practices/recipes/#configurando-o-pythonoperator","text":"","title":"Configurando o PythonOperator"},{"location":"content/building_pipelines/dependencies_between_tasks/","text":"Depend\u00eancias entre Tarefas \u00b6 Introdu\u00e7\u00e3o \u00b6 As depend\u00eancias entre tarefas tem como prop\u00f3sito informar ao Airflow a ordem de execu\u00e7\u00e3o das tarefas de uma DAG. Assim, uma tarefa A s\u00f3 pode ser executada quando suas depend\u00eancias (upstream dependences) tiverem sido executadas (no geral, com sucesso). Como j\u00e1 vimos em Conceitos & Componentes Essenciais , usamos o operador >> ( right bitshift ) para definir a depend\u00eancia entre as tarefas. Assim: Uma tarefa A s\u00f3 ser\u00e1 executada ap\u00f3s suas depend\u00eancias (upstream tasks) Erros de uma tarefa A s\u00e3o propagados para as tarefas posteriores que a possuem como depend\u00eancia (downstream tasks). Depend\u00eancias Lineares \u00b6 A depend\u00eancia linear \u00e9 uma cadeia linear de tasks onde uma task A deve ser completada antes que a task B possa ser executada, uma vez que o resultado da task A \u00e9 utilizado como entrada para a task B. Exemplo Exemplo de DAG formada uma \u00fanica cadeia de depend\u00eancias lineares task_a >> task_b >> task_c Depend\u00eancias Fan-in/out \u00b6 Uma depend\u00eancia do tipo fan-in/out \u00e9 aquela cujo uma tarefa A \u00e9 upstream de m\u00faltiplas tarefas ou possuem muitas tarefas como depend\u00eancias. Mais precisamente, quando uma tarefa A possui m\u00faltiplas depend\u00eancias ent\u00e3o usamos \"fan-in\" . J\u00e1 quando uma tarefa A \u00e9 depend\u00eancia de diversas tarefas, ent\u00e3o usamos \"fan-out\". No caso, podemos usar a sintaxe [<upstream tasks>] >> <downstream task> para definir depend\u00eancias do tipo fan-in/out Exemplo Exemplo de depend\u00eancias fan-in e fan-out Fan-in (muitos-para-um) [ task_x , task_y , task_z ] >> task_a Fan-out (um-para-muitos) task_a >> [ task_x , task_y , task_z ] Branching \u00b6 Branching \u00e9 uma funcionalidade do Airflow que nos permite percorrer um caminho espec\u00edfico da DAG com base em algum crit\u00e9rio condicional. Para tal, usamos o BranchPythonOperator . O BranchPythonOperator , assim como o PythonOperator recebe um objeto Python execut\u00e1vel. A diferen\u00e7a \u00e9 que para o BranchPythonOperator , o execut\u00e1vel deve retornar a task_id (ou uma lista de task_id ) das tarefas que devem ser executadas, enquanto todas as outras tarefas n\u00e3o ser\u00e3o ignoradas (e seus status ser\u00e3o marcados como skipped ). Exemplo TODO: Adicionar figura de exemplo Perceba que se usarmos depends_on_past=True nos poss\u00edveis caminhos da DAG, a partir da primeira execu\u00e7\u00e3o todos os outros caminhos n\u00e3o executados n\u00e3o ser\u00e3o escalonados em qualquer pr\u00f3ximo agendamento. Afinal, como estas tarefas n\u00e3o ter\u00e3o status= Succeed ser\u00e1 imposs\u00edvel execut\u00e1-las. Ainda, caso diferentes caminhos se encaminhem para uma \u00fanica tarefa (fan-in), precisamos alterar o par\u00e2metro trigger_rule . Regras de disparo (trigger rules) s\u00e3o crit\u00e9rios que definem quando uma tarefa pode ser executada. Por exemplo, o padr\u00e3o do argumento trigger_rule \u00e9 all_success , o que significa que uma tarefa s\u00f3 ser\u00e1 executada quando todas as suas depend\u00eancias forem executadas com sucesso. Por\u00e9m, este \u00e9 um caso imposs\u00edvel de acontecer quando lidamos com branching. Consequentemente, a tarefa com depend\u00eancia fan-in ficar\u00e1 em estado de espera eternamente. Uma forma de corrigir este problema \u00e9 alterar a regra de disparo para none_failed que define que uma tarefa deve rodar assim que todas as depend\u00eancias forem executadas sem falhar. Exemplo TODO: Adicionar figura de exemplo Branching Customizado \u00b6 Se quisermos implementar nosso pr\u00f3prio operador de branching, basta criarmos uma classe que herda de BaseBranchOperator e re-implementarmos o m\u00e9todo choose_branch , que \u00e9 o m\u00e9todo chamado no procedimento de branching. Nota O choose_branch deve retornar a task_id (ou lista de task_id) que devem ser executadas. Tarefas Condicionais \u00b6 ShortCircuitOperator \u00b6 Work in progress LatestOnlyOperator \u00b6 Work in progress Regras de Disparo \u00b6 Work in progress Refer\u00eancias \u00b6 Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Airflow Guides by Astronomer Concepts \\(-\\) Apache Airflow Documentation","title":"Depend\u00eancias entre Tarefas"},{"location":"content/building_pipelines/dependencies_between_tasks/#dependencias-entre-tarefas","text":"","title":"Depend\u00eancias entre Tarefas"},{"location":"content/building_pipelines/dependencies_between_tasks/#introducao","text":"As depend\u00eancias entre tarefas tem como prop\u00f3sito informar ao Airflow a ordem de execu\u00e7\u00e3o das tarefas de uma DAG. Assim, uma tarefa A s\u00f3 pode ser executada quando suas depend\u00eancias (upstream dependences) tiverem sido executadas (no geral, com sucesso). Como j\u00e1 vimos em Conceitos & Componentes Essenciais , usamos o operador >> ( right bitshift ) para definir a depend\u00eancia entre as tarefas. Assim: Uma tarefa A s\u00f3 ser\u00e1 executada ap\u00f3s suas depend\u00eancias (upstream tasks) Erros de uma tarefa A s\u00e3o propagados para as tarefas posteriores que a possuem como depend\u00eancia (downstream tasks).","title":"Introdu\u00e7\u00e3o"},{"location":"content/building_pipelines/dependencies_between_tasks/#dependencias-lineares","text":"A depend\u00eancia linear \u00e9 uma cadeia linear de tasks onde uma task A deve ser completada antes que a task B possa ser executada, uma vez que o resultado da task A \u00e9 utilizado como entrada para a task B. Exemplo Exemplo de DAG formada uma \u00fanica cadeia de depend\u00eancias lineares task_a >> task_b >> task_c","title":"Depend\u00eancias Lineares"},{"location":"content/building_pipelines/dependencies_between_tasks/#dependencias-fan-inout","text":"Uma depend\u00eancia do tipo fan-in/out \u00e9 aquela cujo uma tarefa A \u00e9 upstream de m\u00faltiplas tarefas ou possuem muitas tarefas como depend\u00eancias. Mais precisamente, quando uma tarefa A possui m\u00faltiplas depend\u00eancias ent\u00e3o usamos \"fan-in\" . J\u00e1 quando uma tarefa A \u00e9 depend\u00eancia de diversas tarefas, ent\u00e3o usamos \"fan-out\". No caso, podemos usar a sintaxe [<upstream tasks>] >> <downstream task> para definir depend\u00eancias do tipo fan-in/out Exemplo Exemplo de depend\u00eancias fan-in e fan-out Fan-in (muitos-para-um) [ task_x , task_y , task_z ] >> task_a Fan-out (um-para-muitos) task_a >> [ task_x , task_y , task_z ]","title":"Depend\u00eancias Fan-in/out"},{"location":"content/building_pipelines/dependencies_between_tasks/#branching","text":"Branching \u00e9 uma funcionalidade do Airflow que nos permite percorrer um caminho espec\u00edfico da DAG com base em algum crit\u00e9rio condicional. Para tal, usamos o BranchPythonOperator . O BranchPythonOperator , assim como o PythonOperator recebe um objeto Python execut\u00e1vel. A diferen\u00e7a \u00e9 que para o BranchPythonOperator , o execut\u00e1vel deve retornar a task_id (ou uma lista de task_id ) das tarefas que devem ser executadas, enquanto todas as outras tarefas n\u00e3o ser\u00e3o ignoradas (e seus status ser\u00e3o marcados como skipped ). Exemplo TODO: Adicionar figura de exemplo Perceba que se usarmos depends_on_past=True nos poss\u00edveis caminhos da DAG, a partir da primeira execu\u00e7\u00e3o todos os outros caminhos n\u00e3o executados n\u00e3o ser\u00e3o escalonados em qualquer pr\u00f3ximo agendamento. Afinal, como estas tarefas n\u00e3o ter\u00e3o status= Succeed ser\u00e1 imposs\u00edvel execut\u00e1-las. Ainda, caso diferentes caminhos se encaminhem para uma \u00fanica tarefa (fan-in), precisamos alterar o par\u00e2metro trigger_rule . Regras de disparo (trigger rules) s\u00e3o crit\u00e9rios que definem quando uma tarefa pode ser executada. Por exemplo, o padr\u00e3o do argumento trigger_rule \u00e9 all_success , o que significa que uma tarefa s\u00f3 ser\u00e1 executada quando todas as suas depend\u00eancias forem executadas com sucesso. Por\u00e9m, este \u00e9 um caso imposs\u00edvel de acontecer quando lidamos com branching. Consequentemente, a tarefa com depend\u00eancia fan-in ficar\u00e1 em estado de espera eternamente. Uma forma de corrigir este problema \u00e9 alterar a regra de disparo para none_failed que define que uma tarefa deve rodar assim que todas as depend\u00eancias forem executadas sem falhar. Exemplo TODO: Adicionar figura de exemplo","title":"Branching"},{"location":"content/building_pipelines/dependencies_between_tasks/#branching-customizado","text":"Se quisermos implementar nosso pr\u00f3prio operador de branching, basta criarmos uma classe que herda de BaseBranchOperator e re-implementarmos o m\u00e9todo choose_branch , que \u00e9 o m\u00e9todo chamado no procedimento de branching. Nota O choose_branch deve retornar a task_id (ou lista de task_id) que devem ser executadas.","title":"Branching Customizado"},{"location":"content/building_pipelines/dependencies_between_tasks/#tarefas-condicionais","text":"","title":"Tarefas Condicionais"},{"location":"content/building_pipelines/dependencies_between_tasks/#shortcircuitoperator","text":"Work in progress","title":"ShortCircuitOperator"},{"location":"content/building_pipelines/dependencies_between_tasks/#latestonlyoperator","text":"Work in progress","title":"LatestOnlyOperator"},{"location":"content/building_pipelines/dependencies_between_tasks/#regras-de-disparo","text":"Work in progress","title":"Regras de Disparo"},{"location":"content/building_pipelines/dependencies_between_tasks/#referencias","text":"Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Airflow Guides by Astronomer Concepts \\(-\\) Apache Airflow Documentation","title":"Refer\u00eancias"},{"location":"content/building_pipelines/running_tasks_in_containers/","text":"Work in progress","title":"Executando Tarefas em Containers"},{"location":"content/building_pipelines/scheduling_and_sensors/","text":"Agendamento e Sensores \u00b6 Introdu\u00e7\u00e3o \u00b6 DAGs s\u00e3o disparadas de duas formas: Manualmente por meio da UI ou API. Automaticamente por meio de uma pol\u00edtica de disparo. J\u00e1 para que as DAGs sejam \"dispar\u00e1veis\" (i.e. execut\u00e1veis) elas precisam de: Uma data de in\u00edcio a partir do qual podem ser executadas. Uma pol\u00edtica de disparo que pode ser (al\u00e9m do disparo manual): um intervalo de execu\u00e7\u00e3o e/ou um crit\u00e9rio condicional. Essas configura\u00e7\u00f5es s\u00e3o definidas durante a inicializa\u00e7\u00e3o (i.e. cria\u00e7\u00e3o do objeto) da DAG. Configurando o Agendamento de DAGs \u00b6 Chamamos de agendamento (ou escalonamento) o procedimento executado pelo Airflow de definir quando uma DAG deve ser executada. O agendamento \u00e9 configurado atrav\u00e9s de tr\u00eas par\u00e2metros principais: start_date , schedule_interval e end_date . Todos estes par\u00e2metros s\u00e3o atribuidos durante a inicializa\u00e7\u00e3o da DAG. start_date [obrigat\u00f3rio] . Momento a partir do qual a DAG em quest\u00e3o estar\u00e1 dispon\u00edvel para ser executada. Note que o argumento start_date \u00e9 obrigat\u00f3rio durante a inicializa\u00e7\u00e3o pois sem uma data de in\u00edcio \u00e9 imposs\u00edvel para o Airflow saber se a DAG pode ou n\u00e3o ser executada. schedule_interval . Intervalo de execu\u00e7\u00e3o da DAG. Por padr\u00e3o, o valor de schedule_interval \u00e9 None , o que significa que a DAG em quest\u00e3o s\u00f3 ser\u00e1 executada quando triggada manualmente. end_date . Momento at\u00e9 onde a DAG deve ser executada. Abaixo, um exemplo de uma DAG com escalonamento di\u00e1rio. dag = DAG ( dag_id = \"02_daily_schedule\" , schedule_interval = \"@daily\" , start_date = dt . datetime ( 2019 , 1 , 1 ), ... ) Uma vez definida a DAG, o Airflow ir\u00e1 agendar sua primeira execu\u00e7\u00e3o para o primeiro intervalo a partir da data de in\u00edcio. Exemplo Se definirmos que uma DAG est\u00e1 dispon\u00edvel para ser executada a partir do dia 09 de Agosto de 2021 \u00e0s 00hrs com intervalo de execu\u00e7\u00e3o de 15 minutos , a primeira execu\u00e7\u00e3o ser\u00e1 agendada para 00:15, a segunda execu\u00e7\u00e3o ser\u00e1 agendada para 00:30 e assim sucessivamente. Se n\u00e3o definirmos uma data final, o Airflow ir\u00e1 agendar e executar a DAG em quest\u00e3o eternamente. Assim, caso exista uma data final definitiva a partir do qual a DAG n\u00e3o dever\u00e1 ser executada, podemos utilizar o argumento end_date da mesma forma que start_date para limitar os agendamentos. dag = DAG ( dag_id = \"03_with_end_date\" , schedule_interval = \"@daily\" , start_date = dt . datetime ( year = 2019 , month = 1 , day = 1 ), end_date = dt . datetime ( year = 2019 , month = 1 , day = 5 ), ) Intervalos Baseados em Cron \u00b6 Podemos definir intervalos de execu\u00e7\u00e3o complexos usando a mesma sintaxe que usamos no cron . Basicamente, a sintaxe \u00e9 composta por cinco componentes organizados da seguinte forma: # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) # \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) # \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) # \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500 month (1 - 12) # \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday; # \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) # * * * * * Fonte: Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter O car\u00e1cter * significa que o valor do campo em quest\u00e3o n\u00e3o importa. Assim, podemos definir desde intervalos simples e convencionais: 0 * * * * . Executa a cada hora At\u00e9 intervalos mais complexos 0 0 1 * * . Executa a cada primeiro dia do m\u00eas (\u00e0s 00hrs) Tamb\u00e9m podemos utilizar de v\u00edrgulas ( , ) para definir conjuntos de valores e h\u00edfen ( - ) para intervalos de valores. Por exemplo: 0 0 * * MON, WED, FRI . Executa toda segunda, quarta e sexta-feira (\u00e0s 00hrs) 0 0,12 * * MON-FRI . Executa \u00e0s 00hrs e 12hrs de segunda \u00e0 sexta-feira Alternativamente, podemos recorrer \u00e0 ferramentas como crontab.guru e crontab-generator para definirmos as express\u00f5es de forma mais f\u00e1cil. O Airflow tamb\u00e9m fornece alguns macros que podemos utilizar com mais facilidade. Os mais comuns s\u00e3o: Macro Descri\u00e7\u00e3o @once Executa uma \u00fanica vez @hourly Executa a cada come\u00e7o de hora @daily Executa todos os dias \u00e0s 00hrs @weekly Executa todo domingo \u00e0s 00hrs Intervalos Baseados em Frequ\u00eancia \u00b6 Embora poderosas, express\u00f5es cron s\u00e3o incapazes de representar agendamentos baseados em frequ\u00eancia. Por exemplo, n\u00e3o \u00e9 poss\u00edvel definir (de forma adequada) um intervalo de \"tr\u00eas em tr\u00eas em dias\". Por conta disso, o Airflow tamb\u00e9m aceita inst\u00e2ncias timedelta para definir intervalos de execu\u00e7\u00e3o. Com isso, podemos definir uma DAG que \u00e9 executada a cada tr\u00eas dias a partir da data de in\u00edcio. dag = DAG ( dag_id = \"04_time_delta\" , schedule_interval = dt . timedelta ( days = 3 ), start_date = dt . datetime ( year = 2019 , month = 1 , day = 1 ), end_date = dt . datetime ( year = 2019 , month = 1 , day = 5 ), ) Catchup & Backfill \u00b6 Por padr\u00e3o, o Airflow sempre ir\u00e1 escalonar e executar toda e qualquer execu\u00e7\u00e3o passada que deveria ter sido executada mas que por algum motivo n\u00e3o foi . Este comportamento \u00e9 denominado \"backfill\" e \u00e9 controlado pelo argumento catchup \\(-\\) presente na inicializa\u00e7\u00e3o da DAG. Caso este comportamento n\u00e3o seja desejado, basta desativ\u00e1-lo atribuindo False ao par\u00e2metro. Dessa forma, o Airflow ir\u00e1 executar a DAG apenas a partir do primeiro intervalo de execu\u00e7\u00e3o mais pr\u00f3ximo. dag = DAG ( dag_id = \"09_no_catchup\" , schedule_interval = \"@daily\" , start_date = dt . datetime ( year = 2019 , month = 1 , day = 1 ), end_date = dt . datetime ( year = 2019 , month = 1 , day = 5 ), catchup = False , ) Example Se uma DAG for ativada dentro de um per\u00edodo intervalar, o Airflow ir\u00e1 agendar a execu\u00e7\u00e3o da DAG para o in\u00edcio deste per\u00edodo. Por exemplo, considerando uma DAG com start_date=datetime(2021, 08, 10) e schedule_interval=\"@daily\" ; se ativarmos a DAG no dia 15 do mesmo m\u00eas \u00e0s 12hrs, o Airflow ir\u00e1 definir que a primeira execu\u00e7\u00e3o da DAG dever\u00e1 ter ocorrido \u00e0s 00hrs do dia 15 e ir\u00e1 executar a DAG imediatamente. Nota Para mudar o valor padr\u00e3o do catchup de True para False , basta acessar o arquivo de configura\u00e7\u00e3o e modificar o par\u00e2metro catchup_by_default . Embora o backfilling possa ser indesejado em algumas situa\u00e7\u00f5es, seu uso \u00e9 muito \u00fatil para a reexecu\u00e7\u00e3o de tarefas hist\u00f3ricas. Por exemplo, suponha as seguintes tarefas download_data >> process_data . Considerando que os dados adquiridos atrav\u00e9s da tarefa download_data ainda sejam reacess\u00e1veis localmente, podemos realizar as altera\u00e7\u00f5es desejadas em process_data e ent\u00e3o limparmos as execu\u00e7\u00f5es passadas (atrav\u00e9s do bot\u00e3o Clear ) para que assim o Airflow reagende e execute a nova implementa\u00e7\u00e3o de process_data . Aten\u00e7\u00e3o A reexecu\u00e7\u00e3o das DAGs n\u00e3o ocorre de forma ordenada (i.e. de acordo com a data de execu\u00e7\u00e3o), mas sim de forma paralela. Para que o backfilling ocorra de forma ordenada, \u00e9 necess\u00e1rio que o argumento depends_on_past presente na inicializa\u00e7\u00e3o das tarefas seja True . Detalhes sobre o argumento ser\u00e3o apresentados a diante. Datas de execu\u00e7\u00e3o de uma DAG \u00b6 Em diversas situa\u00e7\u00f5es \u00e9 \u00fatil sabermos as datas de execu\u00e7\u00e3o de uma DAG. Por conta disso, o Airflow nos permite acessar tanto a data da execu\u00e7\u00e3o corrente da DAG, quanto a data da execu\u00e7\u00e3o imediatamente anterior e posterior. execution_date. Data da execu\u00e7\u00e3o corrente de DAG. Contudo, diferente do que o nome sugere, a data de execu\u00e7\u00e3o marcada na DAG n\u00e3o \u00e9 a data em que ela foi executada, mas sim no momento em que ela deveria ser executada, com base no intervalo de execu\u00e7\u00e3o. Exemplo Suponha que temos uma DAG configurada para executar diariamente a partir do dia 2020-01-01. Ap\u00f3s dez dias de execu\u00e7\u00e3o, fizemos algumas altera\u00e7\u00f5es de implementa\u00e7\u00e3o e agora precisamos que as execu\u00e7\u00f5es dos \u00faltimos dez dias sejam refeitas. Neste caso, ao acionarmos a op\u00e7\u00e3o Clear , as datas de execu\u00e7\u00e3o da DAG permanecer\u00e3o de acordo com a data em que foram agendadas originalmente. Agora, se triggarmos manualmente a DAG antes da pr\u00f3xima execu\u00e7\u00e3o agendada, a execution_date ser\u00e1 o momento em que a DAG foi de fato disparada. previous_execution_date . Data de execu\u00e7\u00e3o ( execution_date ) da DAG imediatamente anterior. next_execution_date . Pr\u00f3xima data de execu\u00e7\u00e3o ( execution_date ) agendada para a DAG. O acesso a essas informa\u00e7\u00f5es pode ser feito atrav\u00e9s do contexto da DAG[^1] ou por meio dos macros pr\u00e9-definidos: Macros Macro Descri\u00e7\u00e3o {{ ds }} Data de execu\u00e7\u00e3o no formato YYYY-MM-DD {{ ds_nodash }} Data de execu\u00e7\u00e3o no formato YYYYMMDD {{ prev_ds }} Data de execu\u00e7\u00e3o anterior no formato YYYY-MM-DD {{ next_ds }} Pr\u00f3xima data de execu\u00e7\u00e3o no formato YYYY-MM-DD Uso fetch_events = BashOperator ( task_id = \"fetch_events\" , bash_command = ( f \"curl -o /data/events.json { source } \" \"start_date={{ ds }}\" \"end_date={{ next_ds }}\" ), ... ) Sensores \u00b6 Al\u00e9m de execu\u00e7\u00f5es autom\u00e1ticas realizadas em intervalos de tempo, podemos querer disparar uma DAG sempre que um crit\u00e9rio condicional for atendido. No Airflow, podemos fazer isso atrav\u00e9s de Sensores. Um sensor \u00e9 um tipo especial de operador que verifica continuamente (em um intervalo de tempo) se uma certa condi\u00e7\u00e3o \u00e9 verdadeira ou falsa. Se verdadeira, o sensor tem seu estado alterado para bem-sucedido e o restante do pipeline \u00e9 executado. Se falsa, o sensor continua tentando at\u00e9 que a condi\u00e7\u00e3o seja verdadeira ou um tempo limite (timeout) for atingido. Um sensor muito utilizado \u00e9 o FileSensor que verifica a exist\u00eancia de um arquivo e retorna verdadeiro caso o arquivo exista. Caso contr\u00e1rio, o FileSensor retorna False e refaz a checagem ap\u00f3s um intervalo de 60 segundos (valor padr\u00e3o). Este ciclo permanece at\u00e9 que o arquivo venha a existir ou um tempo limite seja atigindo (por padr\u00e3o, 7 dias). Podemos configurar o intervalo de reavali\u00e7\u00e3o atrav\u00e9s do argumento poke_interval que espera receber, em segundos, o per\u00edodo de espera entre cada checagem. J\u00e1 o timeout \u00e9 configurado por meio do argumento timeout . from airflow.sensors.filesystem import FileSensor wait_for_file = FileSensor ( task_id = \"wait_for_file\" , filepath = \"/data/file.csv\" , poke_interval = 10 , # 10 seconds timeout = 5 * 60 , # 5 minutes ) Nota O FileSensor suporta wildcards (e.g. astericos [ * ]), o que nos permite criar padr\u00f5es de correspond\u00eancia nos nomes dos arquivos. Informa\u00e7\u00e3o Chamamos de poking a rotina realizada pelo sensor de execu\u00e7\u00e3o e checagem cont\u00ednua de uma condi\u00e7\u00e3o. Condi\u00e7\u00f5es Personalizadas \u00b6 H\u00e1 diversos cen\u00e1rios em que as condi\u00e7\u00f5es de execu\u00e7\u00e3o de um pipeline s\u00e3o mais complexas do que a exist\u00eancia ou n\u00e3o de um arquivo. Em situa\u00e7\u00f5es como essa, podemos recorrer a uma implementa\u00e7\u00e3o baseada em Python atrav\u00e9s do PythonSensor ou ent\u00e3o criarmos nosso pr\u00f3prio sensor. O PythonSensor assim como o PythonOperator executa uma fun\u00e7\u00e3o Python. Contudo, essa fun\u00e7\u00e3o deve retornar um valor booleano: True indicando que a condi\u00e7\u00e3o foi cumprida, False caso contr\u00e1rio. J\u00e1 para criarmos nosso pr\u00f3prio sensor, basta estendermos a classe BaseSensorOperator . Informa\u00e7\u00f5es sobre a cria\u00e7\u00e3o de componentes personalizados s\u00e3o apresentados na se\u00e7\u00e3o Criando Componentes Personalizados Sensores & Deadlock \u00b6 O tempo limite padr\u00e3o de sete dias dos sensores possui uma falha silenciosa. Suponha uma DAG cujo schedule_interval \u00e9 de um dia. Se ao longo do tempo as condi\u00e7\u00f5es n\u00e3o foram atendidas, teremos um acumulo de DAGs e tarefas para serem executadas consider\u00e1vel. O problema deste cen\u00e1rio \u00e9 que o Airflow possui um limite m\u00e1ximo de tarefas que ele consegue executar paralalemente. Portanto, caso o limite seja atingido, as tarefas ficar\u00e3o bloqueadas e nenhuma execu\u00e7\u00e3o ser\u00e1 feita. Este comportamento \u00e9 denominado sensor deadlock . Embora seja poss\u00edvel aumentar o n\u00famero de tarefas execut\u00e1veis em paralelo, as pr\u00e1ticas recomendadas para evitar esse tipo de situa\u00e7\u00e3o s\u00e3o: Definir um timeout menor que o schedule_interval . Com isso, as execu\u00e7\u00f5es ir\u00e3o falhar antes da pr\u00f3xima come\u00e7ar. Alterar a forma como os sensores s\u00e3o acionados pelo scheduler . Por padr\u00e3o, os sensores trabalham no modo poke (o que pode gerar o deadlock ). Atrav\u00e9s do argumento mode presente na classe do sensor em quest\u00e3o, podemos alterar o acionamento do sensor para reschedule . No modo reschedule , o sensor s\u00f3 permanecer\u00e1 ativo como uma tarefa enquanto estiver fazendo as verifica\u00e7\u00f5es. Ao final da verifica\u00e7\u00e3o, caso o crit\u00e9rio de sucesso n\u00e3o seja cumprido, o scheduler colocar\u00e1 o sensor em estado de espera, liberando assim uma posi\u00e7\u00e3o ( slot ) para outras tarefas serem executadas. Triggando outras DAGs \u00b6 TODO Exemplos \u00b6 Para exemplos e casos de uso dos t\u00f3picos abordados nesta se\u00e7\u00e3o, acesse Estudos de Caso . Para detalhes t\u00e9cnicos e pr\u00e1ticas de uso, acesse Receitas & C\u00f3digos Padr\u00f5es . Refer\u00eancias \u00b6 Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Apache Airflow Documentation Airflow Guides by Astronomer Marc Lamberti Blog","title":"Agendamento e Sensores"},{"location":"content/building_pipelines/scheduling_and_sensors/#agendamento-e-sensores","text":"","title":"Agendamento e Sensores"},{"location":"content/building_pipelines/scheduling_and_sensors/#introducao","text":"DAGs s\u00e3o disparadas de duas formas: Manualmente por meio da UI ou API. Automaticamente por meio de uma pol\u00edtica de disparo. J\u00e1 para que as DAGs sejam \"dispar\u00e1veis\" (i.e. execut\u00e1veis) elas precisam de: Uma data de in\u00edcio a partir do qual podem ser executadas. Uma pol\u00edtica de disparo que pode ser (al\u00e9m do disparo manual): um intervalo de execu\u00e7\u00e3o e/ou um crit\u00e9rio condicional. Essas configura\u00e7\u00f5es s\u00e3o definidas durante a inicializa\u00e7\u00e3o (i.e. cria\u00e7\u00e3o do objeto) da DAG.","title":"Introdu\u00e7\u00e3o"},{"location":"content/building_pipelines/scheduling_and_sensors/#configurando-o-agendamento-de-dags","text":"Chamamos de agendamento (ou escalonamento) o procedimento executado pelo Airflow de definir quando uma DAG deve ser executada. O agendamento \u00e9 configurado atrav\u00e9s de tr\u00eas par\u00e2metros principais: start_date , schedule_interval e end_date . Todos estes par\u00e2metros s\u00e3o atribuidos durante a inicializa\u00e7\u00e3o da DAG. start_date [obrigat\u00f3rio] . Momento a partir do qual a DAG em quest\u00e3o estar\u00e1 dispon\u00edvel para ser executada. Note que o argumento start_date \u00e9 obrigat\u00f3rio durante a inicializa\u00e7\u00e3o pois sem uma data de in\u00edcio \u00e9 imposs\u00edvel para o Airflow saber se a DAG pode ou n\u00e3o ser executada. schedule_interval . Intervalo de execu\u00e7\u00e3o da DAG. Por padr\u00e3o, o valor de schedule_interval \u00e9 None , o que significa que a DAG em quest\u00e3o s\u00f3 ser\u00e1 executada quando triggada manualmente. end_date . Momento at\u00e9 onde a DAG deve ser executada. Abaixo, um exemplo de uma DAG com escalonamento di\u00e1rio. dag = DAG ( dag_id = \"02_daily_schedule\" , schedule_interval = \"@daily\" , start_date = dt . datetime ( 2019 , 1 , 1 ), ... ) Uma vez definida a DAG, o Airflow ir\u00e1 agendar sua primeira execu\u00e7\u00e3o para o primeiro intervalo a partir da data de in\u00edcio. Exemplo Se definirmos que uma DAG est\u00e1 dispon\u00edvel para ser executada a partir do dia 09 de Agosto de 2021 \u00e0s 00hrs com intervalo de execu\u00e7\u00e3o de 15 minutos , a primeira execu\u00e7\u00e3o ser\u00e1 agendada para 00:15, a segunda execu\u00e7\u00e3o ser\u00e1 agendada para 00:30 e assim sucessivamente. Se n\u00e3o definirmos uma data final, o Airflow ir\u00e1 agendar e executar a DAG em quest\u00e3o eternamente. Assim, caso exista uma data final definitiva a partir do qual a DAG n\u00e3o dever\u00e1 ser executada, podemos utilizar o argumento end_date da mesma forma que start_date para limitar os agendamentos. dag = DAG ( dag_id = \"03_with_end_date\" , schedule_interval = \"@daily\" , start_date = dt . datetime ( year = 2019 , month = 1 , day = 1 ), end_date = dt . datetime ( year = 2019 , month = 1 , day = 5 ), )","title":"Configurando o Agendamento de DAGs"},{"location":"content/building_pipelines/scheduling_and_sensors/#intervalos-baseados-em-cron","text":"Podemos definir intervalos de execu\u00e7\u00e3o complexos usando a mesma sintaxe que usamos no cron . Basicamente, a sintaxe \u00e9 composta por cinco componentes organizados da seguinte forma: # \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 minute (0 - 59) # \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2500 hour (0 - 23) # \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500 day of the month (1 - 31) # \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500 month (1 - 12) # \u2502 \u2502 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500 day of the week (0 - 6) (Sunday to Saturday; # \u2502 \u2502 \u2502 \u2502 \u2502 7 is also Sunday on some systems) # * * * * * Fonte: Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter O car\u00e1cter * significa que o valor do campo em quest\u00e3o n\u00e3o importa. Assim, podemos definir desde intervalos simples e convencionais: 0 * * * * . Executa a cada hora At\u00e9 intervalos mais complexos 0 0 1 * * . Executa a cada primeiro dia do m\u00eas (\u00e0s 00hrs) Tamb\u00e9m podemos utilizar de v\u00edrgulas ( , ) para definir conjuntos de valores e h\u00edfen ( - ) para intervalos de valores. Por exemplo: 0 0 * * MON, WED, FRI . Executa toda segunda, quarta e sexta-feira (\u00e0s 00hrs) 0 0,12 * * MON-FRI . Executa \u00e0s 00hrs e 12hrs de segunda \u00e0 sexta-feira Alternativamente, podemos recorrer \u00e0 ferramentas como crontab.guru e crontab-generator para definirmos as express\u00f5es de forma mais f\u00e1cil. O Airflow tamb\u00e9m fornece alguns macros que podemos utilizar com mais facilidade. Os mais comuns s\u00e3o: Macro Descri\u00e7\u00e3o @once Executa uma \u00fanica vez @hourly Executa a cada come\u00e7o de hora @daily Executa todos os dias \u00e0s 00hrs @weekly Executa todo domingo \u00e0s 00hrs","title":"Intervalos Baseados em Cron"},{"location":"content/building_pipelines/scheduling_and_sensors/#intervalos-baseados-em-frequencia","text":"Embora poderosas, express\u00f5es cron s\u00e3o incapazes de representar agendamentos baseados em frequ\u00eancia. Por exemplo, n\u00e3o \u00e9 poss\u00edvel definir (de forma adequada) um intervalo de \"tr\u00eas em tr\u00eas em dias\". Por conta disso, o Airflow tamb\u00e9m aceita inst\u00e2ncias timedelta para definir intervalos de execu\u00e7\u00e3o. Com isso, podemos definir uma DAG que \u00e9 executada a cada tr\u00eas dias a partir da data de in\u00edcio. dag = DAG ( dag_id = \"04_time_delta\" , schedule_interval = dt . timedelta ( days = 3 ), start_date = dt . datetime ( year = 2019 , month = 1 , day = 1 ), end_date = dt . datetime ( year = 2019 , month = 1 , day = 5 ), )","title":"Intervalos Baseados em Frequ\u00eancia"},{"location":"content/building_pipelines/scheduling_and_sensors/#catchup-backfill","text":"Por padr\u00e3o, o Airflow sempre ir\u00e1 escalonar e executar toda e qualquer execu\u00e7\u00e3o passada que deveria ter sido executada mas que por algum motivo n\u00e3o foi . Este comportamento \u00e9 denominado \"backfill\" e \u00e9 controlado pelo argumento catchup \\(-\\) presente na inicializa\u00e7\u00e3o da DAG. Caso este comportamento n\u00e3o seja desejado, basta desativ\u00e1-lo atribuindo False ao par\u00e2metro. Dessa forma, o Airflow ir\u00e1 executar a DAG apenas a partir do primeiro intervalo de execu\u00e7\u00e3o mais pr\u00f3ximo. dag = DAG ( dag_id = \"09_no_catchup\" , schedule_interval = \"@daily\" , start_date = dt . datetime ( year = 2019 , month = 1 , day = 1 ), end_date = dt . datetime ( year = 2019 , month = 1 , day = 5 ), catchup = False , ) Example Se uma DAG for ativada dentro de um per\u00edodo intervalar, o Airflow ir\u00e1 agendar a execu\u00e7\u00e3o da DAG para o in\u00edcio deste per\u00edodo. Por exemplo, considerando uma DAG com start_date=datetime(2021, 08, 10) e schedule_interval=\"@daily\" ; se ativarmos a DAG no dia 15 do mesmo m\u00eas \u00e0s 12hrs, o Airflow ir\u00e1 definir que a primeira execu\u00e7\u00e3o da DAG dever\u00e1 ter ocorrido \u00e0s 00hrs do dia 15 e ir\u00e1 executar a DAG imediatamente. Nota Para mudar o valor padr\u00e3o do catchup de True para False , basta acessar o arquivo de configura\u00e7\u00e3o e modificar o par\u00e2metro catchup_by_default . Embora o backfilling possa ser indesejado em algumas situa\u00e7\u00f5es, seu uso \u00e9 muito \u00fatil para a reexecu\u00e7\u00e3o de tarefas hist\u00f3ricas. Por exemplo, suponha as seguintes tarefas download_data >> process_data . Considerando que os dados adquiridos atrav\u00e9s da tarefa download_data ainda sejam reacess\u00e1veis localmente, podemos realizar as altera\u00e7\u00f5es desejadas em process_data e ent\u00e3o limparmos as execu\u00e7\u00f5es passadas (atrav\u00e9s do bot\u00e3o Clear ) para que assim o Airflow reagende e execute a nova implementa\u00e7\u00e3o de process_data . Aten\u00e7\u00e3o A reexecu\u00e7\u00e3o das DAGs n\u00e3o ocorre de forma ordenada (i.e. de acordo com a data de execu\u00e7\u00e3o), mas sim de forma paralela. Para que o backfilling ocorra de forma ordenada, \u00e9 necess\u00e1rio que o argumento depends_on_past presente na inicializa\u00e7\u00e3o das tarefas seja True . Detalhes sobre o argumento ser\u00e3o apresentados a diante.","title":"Catchup &amp; Backfill"},{"location":"content/building_pipelines/scheduling_and_sensors/#datas-de-execucao-de-uma-dag","text":"Em diversas situa\u00e7\u00f5es \u00e9 \u00fatil sabermos as datas de execu\u00e7\u00e3o de uma DAG. Por conta disso, o Airflow nos permite acessar tanto a data da execu\u00e7\u00e3o corrente da DAG, quanto a data da execu\u00e7\u00e3o imediatamente anterior e posterior. execution_date. Data da execu\u00e7\u00e3o corrente de DAG. Contudo, diferente do que o nome sugere, a data de execu\u00e7\u00e3o marcada na DAG n\u00e3o \u00e9 a data em que ela foi executada, mas sim no momento em que ela deveria ser executada, com base no intervalo de execu\u00e7\u00e3o. Exemplo Suponha que temos uma DAG configurada para executar diariamente a partir do dia 2020-01-01. Ap\u00f3s dez dias de execu\u00e7\u00e3o, fizemos algumas altera\u00e7\u00f5es de implementa\u00e7\u00e3o e agora precisamos que as execu\u00e7\u00f5es dos \u00faltimos dez dias sejam refeitas. Neste caso, ao acionarmos a op\u00e7\u00e3o Clear , as datas de execu\u00e7\u00e3o da DAG permanecer\u00e3o de acordo com a data em que foram agendadas originalmente. Agora, se triggarmos manualmente a DAG antes da pr\u00f3xima execu\u00e7\u00e3o agendada, a execution_date ser\u00e1 o momento em que a DAG foi de fato disparada. previous_execution_date . Data de execu\u00e7\u00e3o ( execution_date ) da DAG imediatamente anterior. next_execution_date . Pr\u00f3xima data de execu\u00e7\u00e3o ( execution_date ) agendada para a DAG. O acesso a essas informa\u00e7\u00f5es pode ser feito atrav\u00e9s do contexto da DAG[^1] ou por meio dos macros pr\u00e9-definidos: Macros Macro Descri\u00e7\u00e3o {{ ds }} Data de execu\u00e7\u00e3o no formato YYYY-MM-DD {{ ds_nodash }} Data de execu\u00e7\u00e3o no formato YYYYMMDD {{ prev_ds }} Data de execu\u00e7\u00e3o anterior no formato YYYY-MM-DD {{ next_ds }} Pr\u00f3xima data de execu\u00e7\u00e3o no formato YYYY-MM-DD Uso fetch_events = BashOperator ( task_id = \"fetch_events\" , bash_command = ( f \"curl -o /data/events.json { source } \" \"start_date={{ ds }}\" \"end_date={{ next_ds }}\" ), ... )","title":"Datas de execu\u00e7\u00e3o de uma DAG"},{"location":"content/building_pipelines/scheduling_and_sensors/#sensores","text":"Al\u00e9m de execu\u00e7\u00f5es autom\u00e1ticas realizadas em intervalos de tempo, podemos querer disparar uma DAG sempre que um crit\u00e9rio condicional for atendido. No Airflow, podemos fazer isso atrav\u00e9s de Sensores. Um sensor \u00e9 um tipo especial de operador que verifica continuamente (em um intervalo de tempo) se uma certa condi\u00e7\u00e3o \u00e9 verdadeira ou falsa. Se verdadeira, o sensor tem seu estado alterado para bem-sucedido e o restante do pipeline \u00e9 executado. Se falsa, o sensor continua tentando at\u00e9 que a condi\u00e7\u00e3o seja verdadeira ou um tempo limite (timeout) for atingido. Um sensor muito utilizado \u00e9 o FileSensor que verifica a exist\u00eancia de um arquivo e retorna verdadeiro caso o arquivo exista. Caso contr\u00e1rio, o FileSensor retorna False e refaz a checagem ap\u00f3s um intervalo de 60 segundos (valor padr\u00e3o). Este ciclo permanece at\u00e9 que o arquivo venha a existir ou um tempo limite seja atigindo (por padr\u00e3o, 7 dias). Podemos configurar o intervalo de reavali\u00e7\u00e3o atrav\u00e9s do argumento poke_interval que espera receber, em segundos, o per\u00edodo de espera entre cada checagem. J\u00e1 o timeout \u00e9 configurado por meio do argumento timeout . from airflow.sensors.filesystem import FileSensor wait_for_file = FileSensor ( task_id = \"wait_for_file\" , filepath = \"/data/file.csv\" , poke_interval = 10 , # 10 seconds timeout = 5 * 60 , # 5 minutes ) Nota O FileSensor suporta wildcards (e.g. astericos [ * ]), o que nos permite criar padr\u00f5es de correspond\u00eancia nos nomes dos arquivos. Informa\u00e7\u00e3o Chamamos de poking a rotina realizada pelo sensor de execu\u00e7\u00e3o e checagem cont\u00ednua de uma condi\u00e7\u00e3o.","title":"Sensores"},{"location":"content/building_pipelines/scheduling_and_sensors/#condicoes-personalizadas","text":"H\u00e1 diversos cen\u00e1rios em que as condi\u00e7\u00f5es de execu\u00e7\u00e3o de um pipeline s\u00e3o mais complexas do que a exist\u00eancia ou n\u00e3o de um arquivo. Em situa\u00e7\u00f5es como essa, podemos recorrer a uma implementa\u00e7\u00e3o baseada em Python atrav\u00e9s do PythonSensor ou ent\u00e3o criarmos nosso pr\u00f3prio sensor. O PythonSensor assim como o PythonOperator executa uma fun\u00e7\u00e3o Python. Contudo, essa fun\u00e7\u00e3o deve retornar um valor booleano: True indicando que a condi\u00e7\u00e3o foi cumprida, False caso contr\u00e1rio. J\u00e1 para criarmos nosso pr\u00f3prio sensor, basta estendermos a classe BaseSensorOperator . Informa\u00e7\u00f5es sobre a cria\u00e7\u00e3o de componentes personalizados s\u00e3o apresentados na se\u00e7\u00e3o Criando Componentes Personalizados","title":"Condi\u00e7\u00f5es Personalizadas"},{"location":"content/building_pipelines/scheduling_and_sensors/#sensores-deadlock","text":"O tempo limite padr\u00e3o de sete dias dos sensores possui uma falha silenciosa. Suponha uma DAG cujo schedule_interval \u00e9 de um dia. Se ao longo do tempo as condi\u00e7\u00f5es n\u00e3o foram atendidas, teremos um acumulo de DAGs e tarefas para serem executadas consider\u00e1vel. O problema deste cen\u00e1rio \u00e9 que o Airflow possui um limite m\u00e1ximo de tarefas que ele consegue executar paralalemente. Portanto, caso o limite seja atingido, as tarefas ficar\u00e3o bloqueadas e nenhuma execu\u00e7\u00e3o ser\u00e1 feita. Este comportamento \u00e9 denominado sensor deadlock . Embora seja poss\u00edvel aumentar o n\u00famero de tarefas execut\u00e1veis em paralelo, as pr\u00e1ticas recomendadas para evitar esse tipo de situa\u00e7\u00e3o s\u00e3o: Definir um timeout menor que o schedule_interval . Com isso, as execu\u00e7\u00f5es ir\u00e3o falhar antes da pr\u00f3xima come\u00e7ar. Alterar a forma como os sensores s\u00e3o acionados pelo scheduler . Por padr\u00e3o, os sensores trabalham no modo poke (o que pode gerar o deadlock ). Atrav\u00e9s do argumento mode presente na classe do sensor em quest\u00e3o, podemos alterar o acionamento do sensor para reschedule . No modo reschedule , o sensor s\u00f3 permanecer\u00e1 ativo como uma tarefa enquanto estiver fazendo as verifica\u00e7\u00f5es. Ao final da verifica\u00e7\u00e3o, caso o crit\u00e9rio de sucesso n\u00e3o seja cumprido, o scheduler colocar\u00e1 o sensor em estado de espera, liberando assim uma posi\u00e7\u00e3o ( slot ) para outras tarefas serem executadas.","title":"Sensores &amp; Deadlock"},{"location":"content/building_pipelines/scheduling_and_sensors/#triggando-outras-dags","text":"TODO","title":"Triggando outras DAGs"},{"location":"content/building_pipelines/scheduling_and_sensors/#exemplos","text":"Para exemplos e casos de uso dos t\u00f3picos abordados nesta se\u00e7\u00e3o, acesse Estudos de Caso . Para detalhes t\u00e9cnicos e pr\u00e1ticas de uso, acesse Receitas & C\u00f3digos Padr\u00f5es .","title":"Exemplos"},{"location":"content/building_pipelines/scheduling_and_sensors/#referencias","text":"Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Apache Airflow Documentation Airflow Guides by Astronomer Marc Lamberti Blog","title":"Refer\u00eancias"},{"location":"content/building_pipelines/templating_tasks/","text":"Jinja Templates \u00b6 Introdu\u00e7\u00e3o \u00b6 O templating \u00e9 uma funcionalidade muito \u00fatil e poderosa para o compartilhamento de informa\u00e7\u00f5es \u00e0 tasks em tempo de execu\u00e7\u00e3o. Atrav\u00e9s dela podemos passar \\(-\\) para cada operador \\(-\\) informa\u00e7\u00f5es como datas de execu\u00e7\u00e3o, nomes de tabela, valores, etc. Jinja Para habilitar o templating, o Airflow utiliza a/o Jinja, uma engine de templating que substitui vari\u00e1veis e/ou express\u00f5es em strings modelos em tempo de execu\u00e7\u00e3o. Seu uso \u00e9 bem simples, atrav\u00e9s de strings modelo (ou, macros). Essencialmente, um macro \u00e9 uma string da forma {{ valor }} onde valor \u00e9 a informa\u00e7\u00e3o que queremos renderizar, seja um objeto ou uma string qualquer. Logo, basta passarmos um macro para qualquer par\u00e2metro (de um operador) que aceite templating e cujo <valor> seja algum padr\u00e3o (i.e. fornecido pelo Airflow) ou pr\u00e9-definido pelo usu\u00e1rio/desenvolvedor. Exemplo O macro {{ ds }} \u00e9 um macro padr\u00e3o do Airflow que renderiza um objeto do tipo pendulum . BashOperator ( task_id = \"print_execution_date\" , bash_command = \"echo Executing DAG on {{ ds }}\" , ) Macros Padr\u00f5es \u00b6 O Airflow fornece uma lista padr\u00e3o de macros que s\u00e3o renderizados em objetos. Voc\u00ea pode consultar a lista completa na documenta\u00e7\u00e3o . Note ainda que cada macro \u00e9, na verdade, um componente do dicion\u00e1rio de contextos da tarefa. Al\u00e9m disso, como estes macros s\u00e3o renderizados em objetos, podemos manipul\u00e1-los como tal, incluindo o uso da nota\u00e7\u00e3o ponto para acessar atributos e m\u00e9todos. Exemplo BashOperator ( task_id = \"print_execution_date\" , bash_command = \"echo Executing DAG on {{ ds.format('dddd') }}\" , ) Visualizando os Templates \u00b6 Podemos visualizar as sa\u00eddas geradas em cada macro (i.e. renderiza\u00e7\u00e3o) tanto pela UI quanto CLI. No caso da CLI, podemos visualiz\u00e1-las sem que seja preciso executar qualquer tasks. Para isso, basta executarmos o comando airflow tasks render passando como par\u00e2metro a dag_id , task_id e uma execution_date . $ airflow tasks render dag run_template 2021 -01-01 # ---------------------------------------------------------- # property: bash_command # ---------------------------------------------------------- echo \"Today is Friday\" # ---------------------------------------------------------- # property: env # ---------------------------------------------------------- None J\u00e1 pela UI, basta acessarmos as op\u00e7\u00f5es da task (clicando na mesma) e acionando o bot\u00e3o Rendered . Argumentos e Scripts Templateable \u00b6 Nem todos os argumentos de um operador podem receber templates. Apenas argumentos presentes no atributo template_fields da classe BaseOperator podem ser renderizados. Ainda, dado que alguns par\u00e2metros (de algunso operadores), como bash_command (do BashOperator ), podem receber scripts, tamb\u00e9m h\u00e1 o atributo template_ext que define quais arquivos podem ser renderizados. Dessa forma, podemos incluir macros nestes arquivos. Logo: template_fields . Quais argumentos do operador s\u00e3o \"templateable\" template_ext . Quais arquivos do operador s\u00e3o \"templateable\" Exemplo No caso do BashOperator , os argumentos \"templateable\" s\u00e3o bash_command e env . J\u00e1 os arquivos s\u00e3o .sh e .bash . Com isso, podemos passar macros tanto em strings para o argumento bash_command . BashOperator ( task_id = \"print_execution_date\" , bash_command = \"echo Executing DAG on {{ ds }}\" , ) Como tamb\u00e9m para arquivos .sh e .bash dag.py BashOperator ( task_id = \"print_execution_date\" , bash_command = \"script.sh\" , ) script.sh echo Executing DAG on {{ ds }} Macros Customizados \u00b6 Work in progress Renderizando Objetos Python Nativos \u00b6 Work in progress","title":"Jinja Templates"},{"location":"content/building_pipelines/templating_tasks/#jinja-templates","text":"","title":"Jinja Templates"},{"location":"content/building_pipelines/templating_tasks/#introducao","text":"O templating \u00e9 uma funcionalidade muito \u00fatil e poderosa para o compartilhamento de informa\u00e7\u00f5es \u00e0 tasks em tempo de execu\u00e7\u00e3o. Atrav\u00e9s dela podemos passar \\(-\\) para cada operador \\(-\\) informa\u00e7\u00f5es como datas de execu\u00e7\u00e3o, nomes de tabela, valores, etc. Jinja Para habilitar o templating, o Airflow utiliza a/o Jinja, uma engine de templating que substitui vari\u00e1veis e/ou express\u00f5es em strings modelos em tempo de execu\u00e7\u00e3o. Seu uso \u00e9 bem simples, atrav\u00e9s de strings modelo (ou, macros). Essencialmente, um macro \u00e9 uma string da forma {{ valor }} onde valor \u00e9 a informa\u00e7\u00e3o que queremos renderizar, seja um objeto ou uma string qualquer. Logo, basta passarmos um macro para qualquer par\u00e2metro (de um operador) que aceite templating e cujo <valor> seja algum padr\u00e3o (i.e. fornecido pelo Airflow) ou pr\u00e9-definido pelo usu\u00e1rio/desenvolvedor. Exemplo O macro {{ ds }} \u00e9 um macro padr\u00e3o do Airflow que renderiza um objeto do tipo pendulum . BashOperator ( task_id = \"print_execution_date\" , bash_command = \"echo Executing DAG on {{ ds }}\" , )","title":"Introdu\u00e7\u00e3o"},{"location":"content/building_pipelines/templating_tasks/#macros-padroes","text":"O Airflow fornece uma lista padr\u00e3o de macros que s\u00e3o renderizados em objetos. Voc\u00ea pode consultar a lista completa na documenta\u00e7\u00e3o . Note ainda que cada macro \u00e9, na verdade, um componente do dicion\u00e1rio de contextos da tarefa. Al\u00e9m disso, como estes macros s\u00e3o renderizados em objetos, podemos manipul\u00e1-los como tal, incluindo o uso da nota\u00e7\u00e3o ponto para acessar atributos e m\u00e9todos. Exemplo BashOperator ( task_id = \"print_execution_date\" , bash_command = \"echo Executing DAG on {{ ds.format('dddd') }}\" , )","title":"Macros Padr\u00f5es"},{"location":"content/building_pipelines/templating_tasks/#visualizando-os-templates","text":"Podemos visualizar as sa\u00eddas geradas em cada macro (i.e. renderiza\u00e7\u00e3o) tanto pela UI quanto CLI. No caso da CLI, podemos visualiz\u00e1-las sem que seja preciso executar qualquer tasks. Para isso, basta executarmos o comando airflow tasks render passando como par\u00e2metro a dag_id , task_id e uma execution_date . $ airflow tasks render dag run_template 2021 -01-01 # ---------------------------------------------------------- # property: bash_command # ---------------------------------------------------------- echo \"Today is Friday\" # ---------------------------------------------------------- # property: env # ---------------------------------------------------------- None J\u00e1 pela UI, basta acessarmos as op\u00e7\u00f5es da task (clicando na mesma) e acionando o bot\u00e3o Rendered .","title":"Visualizando os Templates"},{"location":"content/building_pipelines/templating_tasks/#argumentos-e-scripts-templateable","text":"Nem todos os argumentos de um operador podem receber templates. Apenas argumentos presentes no atributo template_fields da classe BaseOperator podem ser renderizados. Ainda, dado que alguns par\u00e2metros (de algunso operadores), como bash_command (do BashOperator ), podem receber scripts, tamb\u00e9m h\u00e1 o atributo template_ext que define quais arquivos podem ser renderizados. Dessa forma, podemos incluir macros nestes arquivos. Logo: template_fields . Quais argumentos do operador s\u00e3o \"templateable\" template_ext . Quais arquivos do operador s\u00e3o \"templateable\" Exemplo No caso do BashOperator , os argumentos \"templateable\" s\u00e3o bash_command e env . J\u00e1 os arquivos s\u00e3o .sh e .bash . Com isso, podemos passar macros tanto em strings para o argumento bash_command . BashOperator ( task_id = \"print_execution_date\" , bash_command = \"echo Executing DAG on {{ ds }}\" , ) Como tamb\u00e9m para arquivos .sh e .bash dag.py BashOperator ( task_id = \"print_execution_date\" , bash_command = \"script.sh\" , ) script.sh echo Executing DAG on {{ ds }}","title":"Argumentos e Scripts Templateable"},{"location":"content/building_pipelines/templating_tasks/#macros-customizados","text":"Work in progress","title":"Macros Customizados"},{"location":"content/building_pipelines/templating_tasks/#renderizando-objetos-python-nativos","text":"Work in progress","title":"Renderizando Objetos Python Nativos"},{"location":"content/building_pipelines/testing_dags/","text":"Work in progress","title":"Testando DAGs"},{"location":"content/going_deeper/communicating_external_systems/","text":"Work in progress","title":"Comunicando com Sistemas Externos"},{"location":"content/going_deeper/creating_custom_components/","text":"Work in progress","title":"Criando Componentes Personalizados"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/","text":"Apache Airflow Conceitos & Componentes \u00b6 O Airflow \u00e9 uma ferramenta rica em recursos e funcionalidades. Por\u00e9m, como toda ferramenta, o Airflow possui um conjunto de conceitos e componentes fundamentais a partir do qual suas funcionalidades giram em torno. Para explicarmos melhor sobre, vamos recorrer a um exemplo. DAG de Exemplo \u00b6 Abaixo, temos o c\u00f3digo de uma DAG que faz o download e o processamento de dados de lan\u00e7amento de foguetes. # Extracted from Data Pipelines with Apache Airflow (check \"Refer\u00eancias\" section) import json import pathlib import requests import requests.exceptions as requests_exceptions from airflow.models import DAG from airflow.utils.dates import days_ago from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator dag = DAG ( dag_id = \"download_rocket_launches\" , start_date = days_ago ( 2 ), schedule_interval = None , ) download_launches = BashOperator ( task_id = \"download_launches\" , bash_command = \"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\" , dag = dag , ) def _get_pictures (): # Ensure directory exists pathlib . Path ( \"/tmp/images\" ) . mkdir ( parents = True , exist_ok = True ) # Download all pictures in launches.json with open ( \"/tmp/launches.json\" ) as f : launches = json . load ( f ) image_urls = [ launch [ \"image\" ] for launch in launches [ \"results\" ]] for image_url in image_urls : try : response = requests . get ( image_url ) image_filename = image_url . split ( \"/\" )[ - 1 ] target_file = f \"/tmp/images/ { image_filename } \" with open ( target_file , \"wb\" ) as f : f . write ( response . content ) print ( f \"Downloaded { image_url } to { target_file } \" ) except requests_exceptions . MissingSchema : print ( f \" { image_url } appears to be an invalid URL.\" ) except requests_exceptions . ConnectionError : print ( f \"Could not connect to { image_url } .\" ) get_pictures = PythonOperator ( task_id = \"get_pictures\" , python_callable = _get_pictures , dag = dag , ) notify = BashOperator ( task_id = \"notify\" , bash_command = 'echo \"There are now $(ls /tmp/images/ | wc -l) images.\"' , dag = dag , ) download_launches >> get_pictures >> notify Quebrando o c\u00f3digo em etapas, dag = DAG ( dag_id = \"download_rocket_launches\" , start_date = airflow . utils . dates . days_ago ( 14 ), schedule_interval = None , ) Nesta fase, estamos criando um objeto do tipo DAG . Este objeto \u00e9 o ponto de partida para a cria\u00e7\u00e3o do nosso pipeline e o recurso que utilizamos para definir nossos pipelines. Uma DAG tem v\u00e1rios par\u00e2metros, no nosso caso: dag_id [obrigat\u00f3rio] . Identificador da DAG. Deve ser um nome \u00fanico, sem caracteres especiais (com exce\u00e7\u00e3o de underscores ) start_date [obrigat\u00f3rio] . Data a partir do qual a DAG pode ser executada. schedule_interval . Intervalo de execu\u00e7\u00e3o da DAG. Ao atribuirmos None , estamos definindo que a DAG em quest\u00e3o s\u00f3 ser\u00e1 executada quando a triggarmos manualmente. Em seguida, definimos nossa primeira tarefa. download_launches = BashOperator ( task_id = \"download_launches\" , bash_command = \"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\" , dag = dag , ) No caso, declaramos um BashOperator cuja responsabilidade \u00e9 executar um comando Bash . Do ponto de vista do usu\u00e1rio, os operadores s\u00e3o os componentes respons\u00e1veis por executar as tarefas em uma DAG. Operadores s\u00e3o totalmente independentes entre si, o que significa que os dados tratados por um operador s\u00e3o, a priori, desconhecidos pelos outros. Contudo, a fim de construirmos um fluxo de execu\u00e7\u00e3o adequado, precisamos no m\u00ednimo definir uma ordem na qual as tarefas ser\u00e3o executadas. Logicamente, o Airflow nos permite definir a ordem de execu\u00e7\u00e3o das tarefas e chamamos isso de depend\u00eancias . Em nosso c\u00f3digo, definimos as depend\u00eancias no seguinte trecho: download_launches >> get_pictures >> notify Utilizamos o operador bin\u00e1rio \"rshift\" ( >> ) para definirmos as depend\u00eancias entre as tarefas. Com isso, estamos dizend ao Airflow que a tarefa get_pictures s\u00f3 dever\u00e1 ser executada ap\u00f3s download_launches . Afinal, s\u00f3 podemos filtrar as imagens ap\u00f3s as coletarmos! Nota Outro termo comum para o relacionamento entre tasks \u00e9 upstream e downstream . upstream . Tarefa que \u00e9 depend\u00eancia de outra. Por exemplo, download_launches \u00e9 upstream de get_pictures . downstream . A tarefa dependente. Por exemplo, get_pictures \u00e9 downstream de download_launches . O BashOperator possui diversos par\u00e2metros. Os mais importantes s\u00e3o: task_id . Identificador da task (obrigat\u00f3rio) . bash_command . Comando ou script Bash a ser executado (obrigat\u00f3rio) . dag . DAG na qual a tarefa deve estar atrelada. Nota O argumento task_id \u00e9 um campo obrigat\u00f3rio em todos os operadores. Dica Existem diversas formas de fazermos de atrelarmos as tarefas \u00e0 uma DAG. Por exemplo, podemos abstrair o trecho dag=dag se criarmos uma DAG como uma gerenciadora de contexto . J\u00e1 no seguinte trecho: get_pictures = PythonOperator ( task_id = \"get_pictures\" , python_callable = _get_pictures , dag = dag , ) Estamos utilizando o operador PythonOperator \\(-\\) cuja responsabilidade \u00e9 executar uma fun\u00e7\u00e3o Python (ou ent\u00e3o, um m\u00e9todo) \\(-\\) para executar a tarefa de filtragem das fotos coletadas definida na fun\u00e7\u00e3o _get_pictures . Tarefas vs Operadores \u00b6 Operadores (operators) e tarefas (tasks) s\u00e3o um podem parecer a mesma coisa, mas n\u00e3o s\u00e3o! Operadores. Componentes especializados em executar um \u00fanico e espec\u00edfico trabalho dentro do workflow. Por exemplo, temos: BashOperator . Respons\u00e1vel por executar um comando ou script Bash. PythonOperator . Respons\u00e1vel por executar uma fun\u00e7\u00e3o Python. SimpleHTTPOperator. Respons\u00e1vel por fazer uma chamada HTTP \u00e0 um endpoint. De certa forma, podemos pensar que uma DAG simplesmente orquestra a execu\u00e7\u00e3o de uma cole\u00e7\u00e3o de operadores. Ainda, como s\u00e3o os operadores que, conceitualmente, executam as tarefas em si, acabamos usando ambos os termos de forma intercambi\u00e1vel. Tarefas. Componentes que podem ser vistos como \"gerenciadores\" de operadores. De fato, embora do ponto de vista do usu\u00e1rio, tarefas e operadores sejam equivalentes, no Airflow ainda temos o componente task em si, que \u00e9 respons\u00e1vel por gerenciar o estado de opera\u00e7\u00e3o dos operadores. Fonte: Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Triggando a DAG Manualmente \u00b6 Agora que temos o c\u00f3digo da nossa DAG pronto, podemos execut\u00e1-la. Uma boa pr\u00e1tica, \u00e9 testar o c\u00f3digo a fim de encontrar algum problema de sintaxe, por exemplo. Logo, seja o nosso arquivo download_and_process_rocket_data.py a DAG. Basta executarmos $ python download_and_process_rocket_data . py Caso o interpretador n\u00e3o encontre qualquer problema de importa\u00e7\u00e3o ou sintaxe, basta acessarmos a UI do Airflow e triggarmos a DAG manualmente! Lidando com Tasks que Falharam \u00b6 \u00c9 muito comum \\(-\\) principalmente durante a etapa de desenvolvimento \\(-\\) algumas tarefas falharem. Os motivos da falha podem ser diversos, desde erros de sintaxe at\u00e9 quest\u00f5es mais complexas como problemas de conectividade. A primeira a\u00e7\u00e3o a ser tomada neste caso \u00e9 consultar os logs da tarefa que falhou para identificarmos os motivos. Uma vez que os motivos s\u00e3o identificados e os problemas corrigidos, podemos retriggar a DAG a partir da tarefa que falhou! Desse modo, n\u00e3o \u00e9 necess\u00e1rio reexecutarmos tarefas posteriores que foram bem-sucedidas. Basta acessarmos a tarefa que falhou atrav\u00e9s da UI e acionarmos o bot\u00e3o Clear . Com isso, o Airflow ir\u00e1 resetar o estado atual da tarefa para um estado \"escalon\u00e1vel\" e ent\u00e3o reexecut\u00e1-la. Definindo Intervalos de Execu\u00e7\u00e3o Regulares \u00b6 Como visto anteriormente, o Airflow nos fornece um argumento schedule_interval onde podemos especificar o intervalo de execu\u00e7\u00e3o da DAG. Ao atribuirmos None , dizemos ao Airflow que a DAG n\u00e3o possui qualquer intervalo de execu\u00e7\u00e3o e, portanto, ela s\u00f3 ser\u00e1 executada quando a triggarmos manualmente. Alternativamente, podemos utilizar a mesma sintaxe da ferramenta Cronjob para definirmos intervalos de execu\u00e7\u00e3o. Por exemplo, ao atribuirmos @daily ao argumento schedule_interval , estamos dizendo ao Airflow que a DAG em quest\u00e3o deve ser executada diariamente (por padr\u00e3o, o Airflow ir\u00e1 execut\u00e1-la \u00e0s 00hrs). Al\u00e9m disso, o Airflow conta com um comportamento chamado Backfill . Isso significa que o Apache Airflow ir\u00e1 definir todos os hor\u00e1rios em que a DAG em quest\u00e3o deveria ter sido executada a partir da sua data de in\u00edcio (i.e. start_date ) e, nos intervalos onde a DAG n\u00e3o tiver sido executada, o Airflow ir\u00e1 reexecut\u00e1-la. Apache Airflow Principais Conceitos \u00b6 Em resumo: DAG. Cole\u00e7\u00e3o de tarefas a serem executadas e organizadas de uma forma cuja suas rela\u00e7\u00f5es e depend\u00eancias s\u00e3o expressas. DAG Run. Inst\u00e2ncia de um DAG para uma data e hora espec\u00edficas. Task. Define um trabalho a ser executado (atrav\u00e9s de um operador) escrito em Python. Task Instance. An instance of a task - that has been assigned to a DAG and has a state associated with a specific DAG run (i.e for a specific execution_date). Operator. Modelo para a realiza\u00e7\u00e3o de alguma tarefa. DAG (Directed Acyclic Graph) \u00b6 Uma DAG (Directed Acyclic Graph) \u00e9 uma cole\u00e7\u00e3o de tarefas a serem executadas e organizadas de uma forma cuja suas rela\u00e7\u00f5es e depend\u00eancias s\u00e3o expressas. DAGs s\u00e3o: Definidas atrav\u00e9s de um script Python. Podem ser utilizadas na forma de um Gerenciador de Contexto. Os par\u00e2metros mais importantes na declara\u00e7\u00e3o de uma DAG s\u00e3o: dag_id . Identificador do DAG. start_date . Timestamp a partir do qual o scheduler deve escalonar a DAG. Em outras palavras, \u00e9 o par\u00e2metro respons\u00e1vel por definir a data \u00e0 partir da qual a DAG est\u00e1 dispon\u00edvel para execu\u00e7\u00e3o. schedule_interval . Intervalo de execu\u00e7\u00e3o da DAG. default_args . Dicion\u00e1rio com par\u00e2metros padr\u00f5es que devem ser passados \u00e0 todas as tasks Dica Quando todos os operadores possu\u00edrem um conjunto de par\u00e2metros em comum, o uso do defaults_args \u00e9 a forma mais inteligente de evitar duplicidade de c\u00f3digo. Outros par\u00e2metros interessantes de se ter conhecimento s\u00e3o: params . Dicion\u00e1rio de par\u00e2metros \u00e0 n\u00edvel DAG que s\u00e3o disponibilizados em templates e namespaces sob par\u00e2metros. concurrency . N\u00famero de inst\u00e2ncias de tarefas que podem ser executadas paralelamente. on_failure_callback . Uma fun\u00e7\u00e3o a ser chamada quando a DAG (mais precisamente, a DAG Run referente \u00e0 DAG) falhar. on_success_callback . Mesmo comportamento do par\u00e2metro on_falilure_callback , por\u00e9m quando a DAG roda com sucesso. Aten\u00e7\u00e3o! Note que o Airflow carregar\u00e1 apenas objetos DAG globais! No Airflow 2.x, atrav\u00e9s da decorator DAG \u00e9 poss\u00edvel gerar DAGs atrav\u00e9s de fun\u00e7\u00f5es. Assim, qualquer fun\u00e7\u00e3o decorada com @dag retorna um objeto DAG. DAG Run \u00b6 Uma DAG Run \u00e9 uma inst\u00e2ncia de uma DAG que, por sua vez, cont\u00e9m inst\u00e2ncias das tarefas que s\u00e3o executadas para uma execution_date espec\u00edfica. Nota A execution_date \u00e9 a data e hora que a DAG Run e as TaskInstance est\u00e3o sendo (ou foram) executadas. Fazendo uma analogia com a Programa\u00e7\u00e3o Orientada \u00e0 Objetos, podemos pensar nas DAGs como classes e nas DAG Runs como objetos da classe DAG (ou seja, uma inst\u00e2ncia da classe DAG). Argumentos Padr\u00e3o ( default_args ) \u00b6 Os argumentos padr\u00f5es s\u00e3o \u00fateis para aplicar um par\u00e2metro comum a muitos operadores (ou tarefas) atrav\u00e9s de um dicion\u00e1rio passado pelo argumento default_args . default_args = { 'owner' : 'airflow' , 'depends_on_past' : False , 'email' : [ 'airflow@example.com' ], 'email_on_failure' : False , 'email_on_retry' : False , 'retries' : 1 , 'retry_delay' : timedelta ( minutes = 5 ), # 'queue': 'bash_queue', ## # 'pool': 'backfill', ## # 'priority_weight': 10, ## # 'wait_for_downstream': False, # 'sla': timedelta(hours=2), # 'execution_timeout': timedelta(seconds=300), # 'on_failure_callback': some_function, # 'on_success_callback': some_other_function, # 'on_retry_callback': another_function, # 'sla_miss_callback': yet_another_function, # 'trigger_rule': 'all_success' } Os par\u00e2metros mais importantes de serem compartilhados entre as tarefas s\u00e3o: owner . Administrador da tarefa (ou task ). depends_on_past . Define se as tasks da DAG dependem do sucesso de tasks anteriores para execu\u00e7\u00e3o. retries . Quantidade de vezes que uma task deve tentar ser reexecutada, se falhar. retry_delay . Tempo de espera at\u00e9 uma pr\u00f3xima tentativa de re-execu\u00e7\u00e3o. on_failure_callback . Uma fun\u00e7\u00e3o a ser chamada quando a tarefa (mais precisamente, a TaskInstance referente \u00e0 task ) falhar. Nota No caso, um Dicion\u00e1rio de Contexto \u00e9 passado como um \u00fanico par\u00e2metro para esta fun\u00e7\u00e3o. O contexto cont\u00e9m refer\u00eancias a objetos relacionados \u00e0 inst\u00e2ncia da tarefa e est\u00e1 documentado na se\u00e7\u00e3o de macros da API. on_sucess_callback . Mesmo comportamento do par\u00e2metro on_falilure_callback , por\u00e9m executado quando a task DAG roda com sucesso. on_retry_callback Mesmo comportamento do par\u00e2metro on_falilure_callback , por\u00e9m executado .quando ocorre uma tentativa de re-execu\u00e7\u00e3o da tarefa. trigger_rule . Define a regra pela qual as depend\u00eancias s\u00e3o aplicadas para que a tarefa seja acionada. As op\u00e7\u00f5es s\u00e3o: { all_success | all_failed | all_done | one_success | one_failed | none_failed | none_failed_or_skipped | none_skipped | dummy} Padr\u00e3o \u00e9 all_success . Dica As op\u00e7\u00f5es podem ser definidas como string ou usando as constantes definidas na classe est\u00e1tica airflow.utils.TriggerRule Outros par\u00e2metros interessantes de se ter conhecimento s\u00e3o: queue . pool . priority_weight . wait_for_downstream . execution_timeout . sla . sla_miss_callback . Tarefas \u00b6 Uma Task define (atrav\u00e9s de Operadores) uma unidade de trabalho dentro de uma DAG e s\u00e3o representadas como um n\u00f3 na DAG. Relacionamento Entre Tarefas \u00b6 Para definir o relacionamento (i.e. depend\u00eancias) entre tarefas, utilizamos os operadores >> e << . Por exemplo, considere a seguinte DAG com duas tasks : with DAG ( 'my_dag' , start_date = datetime ( 2016 , 1 , 1 )) as dag : task_1 = DummyOperator ( 'task_1' ) task_2 = DummyOperator ( 'task_2' ) task_1 >> task_2 # Define dependencies Assim, a task_1 come\u00e7ar\u00e1 a ser executado enquanto a task_2 espera que task_1 seja conclu\u00edda com sucesso para ent\u00e3o iniciar. Podemos dizer que task_1 est\u00e1 \u00e0 upstream da task_2 e, inversamente, task_2 est\u00e1 \u00e0 downstream da task_1 . Task Instances \u00b6 Assim como DAGs s\u00e3o instanciadas em DAG Runs , Tasks s\u00e3o instanciadas em Tasks Instances . Dentre seus diversos pap\u00e9is, as Tasks Instances s\u00e3o as respons\u00e1veis por representar o estado de uma tarefa \\(-\\) ou seja, em qual etapa do seu ciclo de vida a tarefa se encontra. Os poss\u00edveis estados s\u00e3o: none . A task ainda n\u00e3o foi adicionada \u00e0 fila de execu\u00e7\u00e3o (i.e. escalonada), uma vez que suas depend\u00eancias ainda n\u00e3o foram supridas. scheduled . As depend\u00eancias da task foram supridas e ela pode ser executada. queued . A task foi atrelada \u00e0 um worker pelo Executor e est\u00e1 esperando a disponibilidade do worker para ser executada. running . A task est\u00e1 em execu\u00e7\u00e3o (i.e. sendo executada por um worker ). success . A task foi executada sem erros (logo, com sucesso). failed . A task encontrou erros durante a execu\u00e7\u00e3o e, portanto, falhou. skipped . A task foi \"pulada\" devido \u00e0 algum mecanismo de \"branching\" ou similares. upstream_failed . As depend\u00eancias da task falharam e, portanto, ela n\u00e3o pode ser executada. up_for_retry . A task falhou, mas possui mecanismos de \"tentativas\" definidos e portanto pode ser re-escalonada. up_for_reeschedule . A task \u00e9 um Sensor (mais informa\u00e7\u00f5es em documentos mais avan\u00e7ados) e est\u00e1 no modo scheduled . sensing . A task \u00e9 um Smart Sensor (mais informa\u00e7\u00f5es em documentos mais avan\u00e7ados). removed . A task foi removida da DAG desde sua \u00faltima execu\u00e7\u00e3o. Fonte: Task Instances - Apache Airflow Documentation Operadores \u00b6 Operadores definem uma \u00fanica e exclusiva tarefa em um workflow . Os operadores s\u00e3o geralmente (mas nem sempre) at\u00f4micos, o que significa que podem ser aut\u00f4nomos e n\u00e3o precisam compartilhar recursos com quaisquer outros operadores . Se dois operadores precisam compartilhar informa\u00e7\u00f5es entre si (e.g, nome de arquivo ou uma pequena quantidade de dados), voc\u00ea deve considerar combin\u00e1-los em um \u00fanico operador. Se isso n\u00e3o for poss\u00edvel (ou puder ser evitado de forma alguma), o Airflow tem uma feature para a comunica\u00e7\u00e3o cruzada entre operadores chamada XCom descrita na se\u00e7\u00e3o XComs . Operadores n\u00e3o precisam ser atribu\u00eddos as DAGs imediatamente (anteriormente, a dag era um argumento obrigat\u00f3rio). No entanto, uma vez que um operador \u00e9 atribu\u00eddo a um DAG, ele n\u00e3o pode ser transferido ou n\u00e3o atribu\u00eddo Refer\u00eancias \u00b6 Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Concepts \\(-\\) Apache Airflow Documentation","title":"Conceitos e Componentes Essenciais"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#apache-airflow-conceitos-componentes","text":"O Airflow \u00e9 uma ferramenta rica em recursos e funcionalidades. Por\u00e9m, como toda ferramenta, o Airflow possui um conjunto de conceitos e componentes fundamentais a partir do qual suas funcionalidades giram em torno. Para explicarmos melhor sobre, vamos recorrer a um exemplo.","title":"Apache Airflow Conceitos &amp; Componentes"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#dag-de-exemplo","text":"Abaixo, temos o c\u00f3digo de uma DAG que faz o download e o processamento de dados de lan\u00e7amento de foguetes. # Extracted from Data Pipelines with Apache Airflow (check \"Refer\u00eancias\" section) import json import pathlib import requests import requests.exceptions as requests_exceptions from airflow.models import DAG from airflow.utils.dates import days_ago from airflow.operators.bash import BashOperator from airflow.operators.python import PythonOperator dag = DAG ( dag_id = \"download_rocket_launches\" , start_date = days_ago ( 2 ), schedule_interval = None , ) download_launches = BashOperator ( task_id = \"download_launches\" , bash_command = \"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\" , dag = dag , ) def _get_pictures (): # Ensure directory exists pathlib . Path ( \"/tmp/images\" ) . mkdir ( parents = True , exist_ok = True ) # Download all pictures in launches.json with open ( \"/tmp/launches.json\" ) as f : launches = json . load ( f ) image_urls = [ launch [ \"image\" ] for launch in launches [ \"results\" ]] for image_url in image_urls : try : response = requests . get ( image_url ) image_filename = image_url . split ( \"/\" )[ - 1 ] target_file = f \"/tmp/images/ { image_filename } \" with open ( target_file , \"wb\" ) as f : f . write ( response . content ) print ( f \"Downloaded { image_url } to { target_file } \" ) except requests_exceptions . MissingSchema : print ( f \" { image_url } appears to be an invalid URL.\" ) except requests_exceptions . ConnectionError : print ( f \"Could not connect to { image_url } .\" ) get_pictures = PythonOperator ( task_id = \"get_pictures\" , python_callable = _get_pictures , dag = dag , ) notify = BashOperator ( task_id = \"notify\" , bash_command = 'echo \"There are now $(ls /tmp/images/ | wc -l) images.\"' , dag = dag , ) download_launches >> get_pictures >> notify Quebrando o c\u00f3digo em etapas, dag = DAG ( dag_id = \"download_rocket_launches\" , start_date = airflow . utils . dates . days_ago ( 14 ), schedule_interval = None , ) Nesta fase, estamos criando um objeto do tipo DAG . Este objeto \u00e9 o ponto de partida para a cria\u00e7\u00e3o do nosso pipeline e o recurso que utilizamos para definir nossos pipelines. Uma DAG tem v\u00e1rios par\u00e2metros, no nosso caso: dag_id [obrigat\u00f3rio] . Identificador da DAG. Deve ser um nome \u00fanico, sem caracteres especiais (com exce\u00e7\u00e3o de underscores ) start_date [obrigat\u00f3rio] . Data a partir do qual a DAG pode ser executada. schedule_interval . Intervalo de execu\u00e7\u00e3o da DAG. Ao atribuirmos None , estamos definindo que a DAG em quest\u00e3o s\u00f3 ser\u00e1 executada quando a triggarmos manualmente. Em seguida, definimos nossa primeira tarefa. download_launches = BashOperator ( task_id = \"download_launches\" , bash_command = \"curl -o /tmp/launches.json -L 'https://ll.thespacedevs.com/2.0.0/launch/upcoming'\" , dag = dag , ) No caso, declaramos um BashOperator cuja responsabilidade \u00e9 executar um comando Bash . Do ponto de vista do usu\u00e1rio, os operadores s\u00e3o os componentes respons\u00e1veis por executar as tarefas em uma DAG. Operadores s\u00e3o totalmente independentes entre si, o que significa que os dados tratados por um operador s\u00e3o, a priori, desconhecidos pelos outros. Contudo, a fim de construirmos um fluxo de execu\u00e7\u00e3o adequado, precisamos no m\u00ednimo definir uma ordem na qual as tarefas ser\u00e3o executadas. Logicamente, o Airflow nos permite definir a ordem de execu\u00e7\u00e3o das tarefas e chamamos isso de depend\u00eancias . Em nosso c\u00f3digo, definimos as depend\u00eancias no seguinte trecho: download_launches >> get_pictures >> notify Utilizamos o operador bin\u00e1rio \"rshift\" ( >> ) para definirmos as depend\u00eancias entre as tarefas. Com isso, estamos dizend ao Airflow que a tarefa get_pictures s\u00f3 dever\u00e1 ser executada ap\u00f3s download_launches . Afinal, s\u00f3 podemos filtrar as imagens ap\u00f3s as coletarmos! Nota Outro termo comum para o relacionamento entre tasks \u00e9 upstream e downstream . upstream . Tarefa que \u00e9 depend\u00eancia de outra. Por exemplo, download_launches \u00e9 upstream de get_pictures . downstream . A tarefa dependente. Por exemplo, get_pictures \u00e9 downstream de download_launches . O BashOperator possui diversos par\u00e2metros. Os mais importantes s\u00e3o: task_id . Identificador da task (obrigat\u00f3rio) . bash_command . Comando ou script Bash a ser executado (obrigat\u00f3rio) . dag . DAG na qual a tarefa deve estar atrelada. Nota O argumento task_id \u00e9 um campo obrigat\u00f3rio em todos os operadores. Dica Existem diversas formas de fazermos de atrelarmos as tarefas \u00e0 uma DAG. Por exemplo, podemos abstrair o trecho dag=dag se criarmos uma DAG como uma gerenciadora de contexto . J\u00e1 no seguinte trecho: get_pictures = PythonOperator ( task_id = \"get_pictures\" , python_callable = _get_pictures , dag = dag , ) Estamos utilizando o operador PythonOperator \\(-\\) cuja responsabilidade \u00e9 executar uma fun\u00e7\u00e3o Python (ou ent\u00e3o, um m\u00e9todo) \\(-\\) para executar a tarefa de filtragem das fotos coletadas definida na fun\u00e7\u00e3o _get_pictures .","title":"DAG de Exemplo"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#tarefas-vs-operadores","text":"Operadores (operators) e tarefas (tasks) s\u00e3o um podem parecer a mesma coisa, mas n\u00e3o s\u00e3o! Operadores. Componentes especializados em executar um \u00fanico e espec\u00edfico trabalho dentro do workflow. Por exemplo, temos: BashOperator . Respons\u00e1vel por executar um comando ou script Bash. PythonOperator . Respons\u00e1vel por executar uma fun\u00e7\u00e3o Python. SimpleHTTPOperator. Respons\u00e1vel por fazer uma chamada HTTP \u00e0 um endpoint. De certa forma, podemos pensar que uma DAG simplesmente orquestra a execu\u00e7\u00e3o de uma cole\u00e7\u00e3o de operadores. Ainda, como s\u00e3o os operadores que, conceitualmente, executam as tarefas em si, acabamos usando ambos os termos de forma intercambi\u00e1vel. Tarefas. Componentes que podem ser vistos como \"gerenciadores\" de operadores. De fato, embora do ponto de vista do usu\u00e1rio, tarefas e operadores sejam equivalentes, no Airflow ainda temos o componente task em si, que \u00e9 respons\u00e1vel por gerenciar o estado de opera\u00e7\u00e3o dos operadores. Fonte: Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter","title":"Tarefas vs Operadores"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#triggando-a-dag-manualmente","text":"Agora que temos o c\u00f3digo da nossa DAG pronto, podemos execut\u00e1-la. Uma boa pr\u00e1tica, \u00e9 testar o c\u00f3digo a fim de encontrar algum problema de sintaxe, por exemplo. Logo, seja o nosso arquivo download_and_process_rocket_data.py a DAG. Basta executarmos $ python download_and_process_rocket_data . py Caso o interpretador n\u00e3o encontre qualquer problema de importa\u00e7\u00e3o ou sintaxe, basta acessarmos a UI do Airflow e triggarmos a DAG manualmente!","title":"Triggando a DAG Manualmente"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#lidando-com-tasks-que-falharam","text":"\u00c9 muito comum \\(-\\) principalmente durante a etapa de desenvolvimento \\(-\\) algumas tarefas falharem. Os motivos da falha podem ser diversos, desde erros de sintaxe at\u00e9 quest\u00f5es mais complexas como problemas de conectividade. A primeira a\u00e7\u00e3o a ser tomada neste caso \u00e9 consultar os logs da tarefa que falhou para identificarmos os motivos. Uma vez que os motivos s\u00e3o identificados e os problemas corrigidos, podemos retriggar a DAG a partir da tarefa que falhou! Desse modo, n\u00e3o \u00e9 necess\u00e1rio reexecutarmos tarefas posteriores que foram bem-sucedidas. Basta acessarmos a tarefa que falhou atrav\u00e9s da UI e acionarmos o bot\u00e3o Clear . Com isso, o Airflow ir\u00e1 resetar o estado atual da tarefa para um estado \"escalon\u00e1vel\" e ent\u00e3o reexecut\u00e1-la.","title":"Lidando com Tasks que Falharam"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#definindo-intervalos-de-execucao-regulares","text":"Como visto anteriormente, o Airflow nos fornece um argumento schedule_interval onde podemos especificar o intervalo de execu\u00e7\u00e3o da DAG. Ao atribuirmos None , dizemos ao Airflow que a DAG n\u00e3o possui qualquer intervalo de execu\u00e7\u00e3o e, portanto, ela s\u00f3 ser\u00e1 executada quando a triggarmos manualmente. Alternativamente, podemos utilizar a mesma sintaxe da ferramenta Cronjob para definirmos intervalos de execu\u00e7\u00e3o. Por exemplo, ao atribuirmos @daily ao argumento schedule_interval , estamos dizendo ao Airflow que a DAG em quest\u00e3o deve ser executada diariamente (por padr\u00e3o, o Airflow ir\u00e1 execut\u00e1-la \u00e0s 00hrs). Al\u00e9m disso, o Airflow conta com um comportamento chamado Backfill . Isso significa que o Apache Airflow ir\u00e1 definir todos os hor\u00e1rios em que a DAG em quest\u00e3o deveria ter sido executada a partir da sua data de in\u00edcio (i.e. start_date ) e, nos intervalos onde a DAG n\u00e3o tiver sido executada, o Airflow ir\u00e1 reexecut\u00e1-la.","title":"Definindo Intervalos de Execu\u00e7\u00e3o Regulares"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#apache-airflow-principais-conceitos","text":"Em resumo: DAG. Cole\u00e7\u00e3o de tarefas a serem executadas e organizadas de uma forma cuja suas rela\u00e7\u00f5es e depend\u00eancias s\u00e3o expressas. DAG Run. Inst\u00e2ncia de um DAG para uma data e hora espec\u00edficas. Task. Define um trabalho a ser executado (atrav\u00e9s de um operador) escrito em Python. Task Instance. An instance of a task - that has been assigned to a DAG and has a state associated with a specific DAG run (i.e for a specific execution_date). Operator. Modelo para a realiza\u00e7\u00e3o de alguma tarefa.","title":"Apache Airflow Principais Conceitos"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#dag-directed-acyclic-graph","text":"Uma DAG (Directed Acyclic Graph) \u00e9 uma cole\u00e7\u00e3o de tarefas a serem executadas e organizadas de uma forma cuja suas rela\u00e7\u00f5es e depend\u00eancias s\u00e3o expressas. DAGs s\u00e3o: Definidas atrav\u00e9s de um script Python. Podem ser utilizadas na forma de um Gerenciador de Contexto. Os par\u00e2metros mais importantes na declara\u00e7\u00e3o de uma DAG s\u00e3o: dag_id . Identificador do DAG. start_date . Timestamp a partir do qual o scheduler deve escalonar a DAG. Em outras palavras, \u00e9 o par\u00e2metro respons\u00e1vel por definir a data \u00e0 partir da qual a DAG est\u00e1 dispon\u00edvel para execu\u00e7\u00e3o. schedule_interval . Intervalo de execu\u00e7\u00e3o da DAG. default_args . Dicion\u00e1rio com par\u00e2metros padr\u00f5es que devem ser passados \u00e0 todas as tasks Dica Quando todos os operadores possu\u00edrem um conjunto de par\u00e2metros em comum, o uso do defaults_args \u00e9 a forma mais inteligente de evitar duplicidade de c\u00f3digo. Outros par\u00e2metros interessantes de se ter conhecimento s\u00e3o: params . Dicion\u00e1rio de par\u00e2metros \u00e0 n\u00edvel DAG que s\u00e3o disponibilizados em templates e namespaces sob par\u00e2metros. concurrency . N\u00famero de inst\u00e2ncias de tarefas que podem ser executadas paralelamente. on_failure_callback . Uma fun\u00e7\u00e3o a ser chamada quando a DAG (mais precisamente, a DAG Run referente \u00e0 DAG) falhar. on_success_callback . Mesmo comportamento do par\u00e2metro on_falilure_callback , por\u00e9m quando a DAG roda com sucesso. Aten\u00e7\u00e3o! Note que o Airflow carregar\u00e1 apenas objetos DAG globais! No Airflow 2.x, atrav\u00e9s da decorator DAG \u00e9 poss\u00edvel gerar DAGs atrav\u00e9s de fun\u00e7\u00f5es. Assim, qualquer fun\u00e7\u00e3o decorada com @dag retorna um objeto DAG.","title":"DAG (Directed Acyclic Graph)"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#dag-run","text":"Uma DAG Run \u00e9 uma inst\u00e2ncia de uma DAG que, por sua vez, cont\u00e9m inst\u00e2ncias das tarefas que s\u00e3o executadas para uma execution_date espec\u00edfica. Nota A execution_date \u00e9 a data e hora que a DAG Run e as TaskInstance est\u00e3o sendo (ou foram) executadas. Fazendo uma analogia com a Programa\u00e7\u00e3o Orientada \u00e0 Objetos, podemos pensar nas DAGs como classes e nas DAG Runs como objetos da classe DAG (ou seja, uma inst\u00e2ncia da classe DAG).","title":"DAG Run"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#argumentos-padrao-default_args","text":"Os argumentos padr\u00f5es s\u00e3o \u00fateis para aplicar um par\u00e2metro comum a muitos operadores (ou tarefas) atrav\u00e9s de um dicion\u00e1rio passado pelo argumento default_args . default_args = { 'owner' : 'airflow' , 'depends_on_past' : False , 'email' : [ 'airflow@example.com' ], 'email_on_failure' : False , 'email_on_retry' : False , 'retries' : 1 , 'retry_delay' : timedelta ( minutes = 5 ), # 'queue': 'bash_queue', ## # 'pool': 'backfill', ## # 'priority_weight': 10, ## # 'wait_for_downstream': False, # 'sla': timedelta(hours=2), # 'execution_timeout': timedelta(seconds=300), # 'on_failure_callback': some_function, # 'on_success_callback': some_other_function, # 'on_retry_callback': another_function, # 'sla_miss_callback': yet_another_function, # 'trigger_rule': 'all_success' } Os par\u00e2metros mais importantes de serem compartilhados entre as tarefas s\u00e3o: owner . Administrador da tarefa (ou task ). depends_on_past . Define se as tasks da DAG dependem do sucesso de tasks anteriores para execu\u00e7\u00e3o. retries . Quantidade de vezes que uma task deve tentar ser reexecutada, se falhar. retry_delay . Tempo de espera at\u00e9 uma pr\u00f3xima tentativa de re-execu\u00e7\u00e3o. on_failure_callback . Uma fun\u00e7\u00e3o a ser chamada quando a tarefa (mais precisamente, a TaskInstance referente \u00e0 task ) falhar. Nota No caso, um Dicion\u00e1rio de Contexto \u00e9 passado como um \u00fanico par\u00e2metro para esta fun\u00e7\u00e3o. O contexto cont\u00e9m refer\u00eancias a objetos relacionados \u00e0 inst\u00e2ncia da tarefa e est\u00e1 documentado na se\u00e7\u00e3o de macros da API. on_sucess_callback . Mesmo comportamento do par\u00e2metro on_falilure_callback , por\u00e9m executado quando a task DAG roda com sucesso. on_retry_callback Mesmo comportamento do par\u00e2metro on_falilure_callback , por\u00e9m executado .quando ocorre uma tentativa de re-execu\u00e7\u00e3o da tarefa. trigger_rule . Define a regra pela qual as depend\u00eancias s\u00e3o aplicadas para que a tarefa seja acionada. As op\u00e7\u00f5es s\u00e3o: { all_success | all_failed | all_done | one_success | one_failed | none_failed | none_failed_or_skipped | none_skipped | dummy} Padr\u00e3o \u00e9 all_success . Dica As op\u00e7\u00f5es podem ser definidas como string ou usando as constantes definidas na classe est\u00e1tica airflow.utils.TriggerRule Outros par\u00e2metros interessantes de se ter conhecimento s\u00e3o: queue . pool . priority_weight . wait_for_downstream . execution_timeout . sla . sla_miss_callback .","title":"Argumentos Padr\u00e3o (default_args)"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#tarefas","text":"Uma Task define (atrav\u00e9s de Operadores) uma unidade de trabalho dentro de uma DAG e s\u00e3o representadas como um n\u00f3 na DAG.","title":"Tarefas"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#relacionamento-entre-tarefas","text":"Para definir o relacionamento (i.e. depend\u00eancias) entre tarefas, utilizamos os operadores >> e << . Por exemplo, considere a seguinte DAG com duas tasks : with DAG ( 'my_dag' , start_date = datetime ( 2016 , 1 , 1 )) as dag : task_1 = DummyOperator ( 'task_1' ) task_2 = DummyOperator ( 'task_2' ) task_1 >> task_2 # Define dependencies Assim, a task_1 come\u00e7ar\u00e1 a ser executado enquanto a task_2 espera que task_1 seja conclu\u00edda com sucesso para ent\u00e3o iniciar. Podemos dizer que task_1 est\u00e1 \u00e0 upstream da task_2 e, inversamente, task_2 est\u00e1 \u00e0 downstream da task_1 .","title":"Relacionamento Entre Tarefas"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#task-instances","text":"Assim como DAGs s\u00e3o instanciadas em DAG Runs , Tasks s\u00e3o instanciadas em Tasks Instances . Dentre seus diversos pap\u00e9is, as Tasks Instances s\u00e3o as respons\u00e1veis por representar o estado de uma tarefa \\(-\\) ou seja, em qual etapa do seu ciclo de vida a tarefa se encontra. Os poss\u00edveis estados s\u00e3o: none . A task ainda n\u00e3o foi adicionada \u00e0 fila de execu\u00e7\u00e3o (i.e. escalonada), uma vez que suas depend\u00eancias ainda n\u00e3o foram supridas. scheduled . As depend\u00eancias da task foram supridas e ela pode ser executada. queued . A task foi atrelada \u00e0 um worker pelo Executor e est\u00e1 esperando a disponibilidade do worker para ser executada. running . A task est\u00e1 em execu\u00e7\u00e3o (i.e. sendo executada por um worker ). success . A task foi executada sem erros (logo, com sucesso). failed . A task encontrou erros durante a execu\u00e7\u00e3o e, portanto, falhou. skipped . A task foi \"pulada\" devido \u00e0 algum mecanismo de \"branching\" ou similares. upstream_failed . As depend\u00eancias da task falharam e, portanto, ela n\u00e3o pode ser executada. up_for_retry . A task falhou, mas possui mecanismos de \"tentativas\" definidos e portanto pode ser re-escalonada. up_for_reeschedule . A task \u00e9 um Sensor (mais informa\u00e7\u00f5es em documentos mais avan\u00e7ados) e est\u00e1 no modo scheduled . sensing . A task \u00e9 um Smart Sensor (mais informa\u00e7\u00f5es em documentos mais avan\u00e7ados). removed . A task foi removida da DAG desde sua \u00faltima execu\u00e7\u00e3o. Fonte: Task Instances - Apache Airflow Documentation","title":"Task Instances"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#operadores","text":"Operadores definem uma \u00fanica e exclusiva tarefa em um workflow . Os operadores s\u00e3o geralmente (mas nem sempre) at\u00f4micos, o que significa que podem ser aut\u00f4nomos e n\u00e3o precisam compartilhar recursos com quaisquer outros operadores . Se dois operadores precisam compartilhar informa\u00e7\u00f5es entre si (e.g, nome de arquivo ou uma pequena quantidade de dados), voc\u00ea deve considerar combin\u00e1-los em um \u00fanico operador. Se isso n\u00e3o for poss\u00edvel (ou puder ser evitado de forma alguma), o Airflow tem uma feature para a comunica\u00e7\u00e3o cruzada entre operadores chamada XCom descrita na se\u00e7\u00e3o XComs . Operadores n\u00e3o precisam ser atribu\u00eddos as DAGs imediatamente (anteriormente, a dag era um argumento obrigat\u00f3rio). No entanto, uma vez que um operador \u00e9 atribu\u00eddo a um DAG, ele n\u00e3o pode ser transferido ou n\u00e3o atribu\u00eddo","title":"Operadores"},{"location":"content/introduction_to_apache_airflow/essential_concepts_and_components/#referencias","text":"Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Concepts \\(-\\) Apache Airflow Documentation","title":"Refer\u00eancias"},{"location":"content/introduction_to_apache_airflow/introduction_to_apache_airflow/","text":"Introdu\u00e7\u00e3o ao Apache Airflow \u00b6 O Apache Airflow \u00e9 uma plataforma para projetar, construir e monitorar fluxos de trabalho (ou workflows ) atrav\u00e9s de scripts Python. O Airflow possui quatro caracter\u00edsticas principais: Din\u00e2mico. Os pipelines (sin\u00f4nimo para workflows ) no Airflow s\u00e3o como configura\u00e7\u00f5es definidas em c\u00f3digo. Com isso, \u00e9 poss\u00edvel criar pipelines din\u00e2micos e adaptativos de maneira simplificada. Extens\u00edvel. O Airflow permite ao usu\u00e1rio definir os pr\u00f3prios componentes e/ou estender componentes j\u00e1 existem para alcan\u00e7ar as funcionalidades desejadas. Elegante. Os pipelines no Airflow possuem uma declara\u00e7\u00e3o limpa e expl\u00edcita. Escal\u00e1vel. O Airflow possui uma arquitetura modular e usa mensageiros para orquestrar um n\u00famero indefinido de workers , possibilitando uma grande escalabilidade tanto vertical quanto horinzotal. Ainda, essas caracter\u00edsticas s\u00e3o mais do que caracter\u00edsticas. Na verdade, s\u00e3o princ\u00edpios que guiam tanto as funcionalidades quanto o desenvolvimento do Aiflow em si. Por fim, \u00e9 importante ressaltar que o Airflow n\u00e3o \u00e9 uma solu\u00e7\u00e3o de streaming de dados, como \u00e9 o caso de Spark Streaming ou Storm! O Airflow \u00e9 uma ferramenta para a orquestra\u00e7\u00e3o de tarefas na forma de DAGs e, portanto, deve ser utilizado para tal. Arquitetura do Apache Airflow \u00b6 O Airflow \u00e9 composto arquiteturalmente por 4 componentes: Webserver (UI). Interface gr\u00e1fica do Airflow via web. Nada mais \u00e9 que uma aplica\u00e7\u00e3o Flask executada sobre gunicorn que nos fornece uma interface gr\u00e1fica para intera\u00e7\u00e3o com o banco de metadados e arquivos de log. Scheduler. Processo respons\u00e1vel pelo escalonamento das tarefas que comp\u00f5em as DAGs. Essencialmente um processo Python multi-thread que define, atrav\u00e9s do banco de metadados, quais tarefas devem ser executadas, quando e onde. Executor. Componente atrav\u00e9s do qual as tarefas s\u00e3o, de fato, executadas. O Airflow cont\u00e9m uma grande diversidade de executos, cada um com suas vantagens e desvantagens. Metadata Database. Banco de dados (por simplicidade, vamos usar a terminologia \"banco de metadados\") que armazena todas as informa\u00e7\u00f5es, metadados e status das DAGs e tarefas. \u00c9 o componente atrav\u00e9s do qual os demais componentes interagem entre si. Fluxo de Execu\u00e7\u00e3o das Tarefa \u00b6 Para entendermos melhor como o Airflow funciona e o papel de cada componente da arquitetura na execu\u00e7\u00e3o de pipelines, precisamos entender o fluxo de execu\u00e7\u00e3o das DAGs suas respectivas tarefas. Essencialmente, a partir do momento em que o scheduler \u00e9 iniciado: O Scheduler \"l\u00ea\" o diret\u00f3rio de DAGs e instancia todos os objetos DAG no banco de metadados. Nota Isso significa que todos os c\u00f3digos top-level \\(-\\) mesmo que n\u00e3o definam DAGs \\(-\\) ser\u00e3o lidos pelo scheduler, o que pode causar problemas de desempenho. Portanto, devemos evitar c\u00f3digos desnecess\u00e1rios no diret\u00f3rio de DAGs. Atrav\u00e9s do processo de instancia\u00e7\u00e3o (citado acima), todas as Dag Runs 1 necess\u00e1rias s\u00e3o criadas de acordo com os par\u00e2metros de agendamento (ou escalonamento) das tarefas de cada DAG. Para cada tarefa que precisa ser executada, s\u00e3o instanciadas TaskInstances 2 que s\u00e3o marcadas como Scheduled no banco de metadados (al\u00e9m de outros metadados necess\u00e1rios para a execu\u00e7\u00e3o das tarefas). O Scheduler consulta todas as tasks marcadas como Scheduled e as envia para o Executor , atualizando seu status para Queued O Executor puxa as tarefas da fila de execu\u00e7\u00e3o e aloca workers para execut\u00e1-las, alterando seu status de Queued para Running Nota O comportamento de como as tarefas s\u00e3o puxadas da fila e executadas difere para cada Executor escolhido. A tarefa, ao ser encerrada, tem seu status alterado pelo worker para seu estado final (e.g., Finised , Failed ) que ent\u00e3o informa ao Scheduler que, por sua vez, reflete as mudan\u00e7as no banco de metadados. A Figura abaixo ilustra o processo especificado. Fonte: Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Controle de Intera\u00e7\u00f5es Entre os Componente \u00b6 A forma como os componentes se comportam e/ou interagem entre si pode ser definida atrav\u00e9s do arquivo de configura\u00e7\u00e3o airflow.cfg . Executor \u00b6 As op\u00e7\u00f5es de Executores padr\u00f5es para o Airflow s\u00e3o: SequentialExecutor . Executa tarefas sequencialmente, sem paralelismo. \u00datil em ambientes de teste ou para solu\u00e7\u00e3o de bugs complexos. LocalExecutor . Executa tarefas com suporte \u00e0 paralelismo e hyperthreading. Uma boa op\u00e7\u00e3o em cen\u00e1rios onde o Airflow est\u00e1 em uma m\u00e1quina local ou em um \u00fanico n\u00f3. CeleryExecutor . Op\u00e7\u00e3o para execu\u00e7\u00e3o do Airflow em clusters distribu\u00eddos que utiliza de Redis, RabbitMq ou outro sistema de fila de mensagens para coordenar o envio de tarefas aos workers . KubernetesExecutor . Op\u00e7\u00e3o para execu\u00e7\u00e3o do Airflow em clusters Kubernetes. Executa tarefas atrav\u00e9s da cria\u00e7\u00e3o de um pod tempor\u00e1rio para cada tarefa a ser executada, permitindo que os usu\u00e1rios passem configura\u00e7\u00f5es personalizadas para cada uma de suas tarefas e usem os recursos do cluster de maneira eficiente. Paralelismo \u00b6 Atrav\u00e9s dos par\u00e2metros parallelism , dag_concurrency e max_active_runs_per_dag do arquivo airflow.cfg podemos configurar o Airflow para determinar quantas tarefas podem ser executadas simultaneamente. parallelism . N\u00famero m\u00e1ximo de inst\u00e2ncias de tarefas que podem ser executadas simultaneamente. Nota O n\u00famero m\u00e1ximo compreende a soma de tarefas de todas as DAGs. Em outras palavras, o parallelism informa ao Airflow quantas tarefas ele pode executar simultaneamente, considerando todas as DAGs ativas. dag_concurrency . N\u00famero m\u00e1ximo de inst\u00e2ncias de tarefas que podem ser executadas simultaneamente em cada DAG. worker_concurrency ( CeleryExecutor ). N\u00famero m\u00e1ximo de tarefas que um \u00fanico worker pode processar. Portanto, se voc\u00ea tiver 4 workers em execu\u00e7\u00e3o e worker_concurrency=16 , poder\u00e1 processar at\u00e9 64 tarefas simultaneamente. max_active_runs_per_dag . N\u00famero m\u00e1ximo de DagRuns ao longo do tempo podem ser escalonadas para cada DAG espec\u00edfica. Nota Esse n\u00famero deve depender de quanto tempo as DAGs levam para executar, seu intervalo de escalonamento e desempenho do scheduler . Refer\u00eancias \u00b6 Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Airflow Guides by Astronomer Concepts \\(-\\) Apache Airflow Documentation DAG Runs s\u00e3o instancias de uma DAG. \u00c9 um conceito importante no Airflow e \u00e9 abordado com mais detalhes na se\u00e7\u00e3o Apache Airflow Conceitos & Componentes \u21a9 TaskInstances s\u00e3o instancias de tarefas atreladas a uma DAG. Assim como as DAG Runs, tamb\u00e9m s\u00e3o um conceito importante no Airflow e, portanto, abordadas com mais detalhes na se\u00e7\u00e3o Apache Airflow Conceitos & Componentes . \u21a9","title":"Introdu\u00e7\u00e3o"},{"location":"content/introduction_to_apache_airflow/introduction_to_apache_airflow/#introducao-ao-apache-airflow","text":"O Apache Airflow \u00e9 uma plataforma para projetar, construir e monitorar fluxos de trabalho (ou workflows ) atrav\u00e9s de scripts Python. O Airflow possui quatro caracter\u00edsticas principais: Din\u00e2mico. Os pipelines (sin\u00f4nimo para workflows ) no Airflow s\u00e3o como configura\u00e7\u00f5es definidas em c\u00f3digo. Com isso, \u00e9 poss\u00edvel criar pipelines din\u00e2micos e adaptativos de maneira simplificada. Extens\u00edvel. O Airflow permite ao usu\u00e1rio definir os pr\u00f3prios componentes e/ou estender componentes j\u00e1 existem para alcan\u00e7ar as funcionalidades desejadas. Elegante. Os pipelines no Airflow possuem uma declara\u00e7\u00e3o limpa e expl\u00edcita. Escal\u00e1vel. O Airflow possui uma arquitetura modular e usa mensageiros para orquestrar um n\u00famero indefinido de workers , possibilitando uma grande escalabilidade tanto vertical quanto horinzotal. Ainda, essas caracter\u00edsticas s\u00e3o mais do que caracter\u00edsticas. Na verdade, s\u00e3o princ\u00edpios que guiam tanto as funcionalidades quanto o desenvolvimento do Aiflow em si. Por fim, \u00e9 importante ressaltar que o Airflow n\u00e3o \u00e9 uma solu\u00e7\u00e3o de streaming de dados, como \u00e9 o caso de Spark Streaming ou Storm! O Airflow \u00e9 uma ferramenta para a orquestra\u00e7\u00e3o de tarefas na forma de DAGs e, portanto, deve ser utilizado para tal.","title":"Introdu\u00e7\u00e3o ao Apache Airflow"},{"location":"content/introduction_to_apache_airflow/introduction_to_apache_airflow/#arquitetura-do-apache-airflow","text":"O Airflow \u00e9 composto arquiteturalmente por 4 componentes: Webserver (UI). Interface gr\u00e1fica do Airflow via web. Nada mais \u00e9 que uma aplica\u00e7\u00e3o Flask executada sobre gunicorn que nos fornece uma interface gr\u00e1fica para intera\u00e7\u00e3o com o banco de metadados e arquivos de log. Scheduler. Processo respons\u00e1vel pelo escalonamento das tarefas que comp\u00f5em as DAGs. Essencialmente um processo Python multi-thread que define, atrav\u00e9s do banco de metadados, quais tarefas devem ser executadas, quando e onde. Executor. Componente atrav\u00e9s do qual as tarefas s\u00e3o, de fato, executadas. O Airflow cont\u00e9m uma grande diversidade de executos, cada um com suas vantagens e desvantagens. Metadata Database. Banco de dados (por simplicidade, vamos usar a terminologia \"banco de metadados\") que armazena todas as informa\u00e7\u00f5es, metadados e status das DAGs e tarefas. \u00c9 o componente atrav\u00e9s do qual os demais componentes interagem entre si.","title":"Arquitetura do Apache Airflow"},{"location":"content/introduction_to_apache_airflow/introduction_to_apache_airflow/#fluxo-de-execucao-das-tarefa","text":"Para entendermos melhor como o Airflow funciona e o papel de cada componente da arquitetura na execu\u00e7\u00e3o de pipelines, precisamos entender o fluxo de execu\u00e7\u00e3o das DAGs suas respectivas tarefas. Essencialmente, a partir do momento em que o scheduler \u00e9 iniciado: O Scheduler \"l\u00ea\" o diret\u00f3rio de DAGs e instancia todos os objetos DAG no banco de metadados. Nota Isso significa que todos os c\u00f3digos top-level \\(-\\) mesmo que n\u00e3o definam DAGs \\(-\\) ser\u00e3o lidos pelo scheduler, o que pode causar problemas de desempenho. Portanto, devemos evitar c\u00f3digos desnecess\u00e1rios no diret\u00f3rio de DAGs. Atrav\u00e9s do processo de instancia\u00e7\u00e3o (citado acima), todas as Dag Runs 1 necess\u00e1rias s\u00e3o criadas de acordo com os par\u00e2metros de agendamento (ou escalonamento) das tarefas de cada DAG. Para cada tarefa que precisa ser executada, s\u00e3o instanciadas TaskInstances 2 que s\u00e3o marcadas como Scheduled no banco de metadados (al\u00e9m de outros metadados necess\u00e1rios para a execu\u00e7\u00e3o das tarefas). O Scheduler consulta todas as tasks marcadas como Scheduled e as envia para o Executor , atualizando seu status para Queued O Executor puxa as tarefas da fila de execu\u00e7\u00e3o e aloca workers para execut\u00e1-las, alterando seu status de Queued para Running Nota O comportamento de como as tarefas s\u00e3o puxadas da fila e executadas difere para cada Executor escolhido. A tarefa, ao ser encerrada, tem seu status alterado pelo worker para seu estado final (e.g., Finised , Failed ) que ent\u00e3o informa ao Scheduler que, por sua vez, reflete as mudan\u00e7as no banco de metadados. A Figura abaixo ilustra o processo especificado. Fonte: Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter","title":"Fluxo de Execu\u00e7\u00e3o das Tarefa"},{"location":"content/introduction_to_apache_airflow/introduction_to_apache_airflow/#controle-de-interacoes-entre-os-componente","text":"A forma como os componentes se comportam e/ou interagem entre si pode ser definida atrav\u00e9s do arquivo de configura\u00e7\u00e3o airflow.cfg .","title":"Controle de Intera\u00e7\u00f5es Entre os Componente"},{"location":"content/introduction_to_apache_airflow/introduction_to_apache_airflow/#executor","text":"As op\u00e7\u00f5es de Executores padr\u00f5es para o Airflow s\u00e3o: SequentialExecutor . Executa tarefas sequencialmente, sem paralelismo. \u00datil em ambientes de teste ou para solu\u00e7\u00e3o de bugs complexos. LocalExecutor . Executa tarefas com suporte \u00e0 paralelismo e hyperthreading. Uma boa op\u00e7\u00e3o em cen\u00e1rios onde o Airflow est\u00e1 em uma m\u00e1quina local ou em um \u00fanico n\u00f3. CeleryExecutor . Op\u00e7\u00e3o para execu\u00e7\u00e3o do Airflow em clusters distribu\u00eddos que utiliza de Redis, RabbitMq ou outro sistema de fila de mensagens para coordenar o envio de tarefas aos workers . KubernetesExecutor . Op\u00e7\u00e3o para execu\u00e7\u00e3o do Airflow em clusters Kubernetes. Executa tarefas atrav\u00e9s da cria\u00e7\u00e3o de um pod tempor\u00e1rio para cada tarefa a ser executada, permitindo que os usu\u00e1rios passem configura\u00e7\u00f5es personalizadas para cada uma de suas tarefas e usem os recursos do cluster de maneira eficiente.","title":"Executor"},{"location":"content/introduction_to_apache_airflow/introduction_to_apache_airflow/#paralelismo","text":"Atrav\u00e9s dos par\u00e2metros parallelism , dag_concurrency e max_active_runs_per_dag do arquivo airflow.cfg podemos configurar o Airflow para determinar quantas tarefas podem ser executadas simultaneamente. parallelism . N\u00famero m\u00e1ximo de inst\u00e2ncias de tarefas que podem ser executadas simultaneamente. Nota O n\u00famero m\u00e1ximo compreende a soma de tarefas de todas as DAGs. Em outras palavras, o parallelism informa ao Airflow quantas tarefas ele pode executar simultaneamente, considerando todas as DAGs ativas. dag_concurrency . N\u00famero m\u00e1ximo de inst\u00e2ncias de tarefas que podem ser executadas simultaneamente em cada DAG. worker_concurrency ( CeleryExecutor ). N\u00famero m\u00e1ximo de tarefas que um \u00fanico worker pode processar. Portanto, se voc\u00ea tiver 4 workers em execu\u00e7\u00e3o e worker_concurrency=16 , poder\u00e1 processar at\u00e9 64 tarefas simultaneamente. max_active_runs_per_dag . N\u00famero m\u00e1ximo de DagRuns ao longo do tempo podem ser escalonadas para cada DAG espec\u00edfica. Nota Esse n\u00famero deve depender de quanto tempo as DAGs levam para executar, seu intervalo de escalonamento e desempenho do scheduler .","title":"Paralelismo"},{"location":"content/introduction_to_apache_airflow/introduction_to_apache_airflow/#referencias","text":"Data Pipelines with Apache Airflow (2021) by Bas Harenslak and Julian de Ruiter Airflow Guides by Astronomer Concepts \\(-\\) Apache Airflow Documentation DAG Runs s\u00e3o instancias de uma DAG. \u00c9 um conceito importante no Airflow e \u00e9 abordado com mais detalhes na se\u00e7\u00e3o Apache Airflow Conceitos & Componentes \u21a9 TaskInstances s\u00e3o instancias de tarefas atreladas a uma DAG. Assim como as DAG Runs, tamb\u00e9m s\u00e3o um conceito importante no Airflow e, portanto, abordadas com mais detalhes na se\u00e7\u00e3o Apache Airflow Conceitos & Componentes . \u21a9","title":"Refer\u00eancias"}]}